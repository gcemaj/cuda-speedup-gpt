,gpu_src,cpu_src,cpu_exec,gpu_exec,errors,n,b1,b2,b3,g1,g2,g3
0,"


__global__ void kernel(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int angry_ardinghelli = bx*bdx + tx;
    if (angry_ardinghelli < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * musing_beaver_input_gpu;
    float * inspiring_shaw_output_gpu;
    float * sad_poincare_output_gpu;

    cudaMalloc(&musing_beaver_input_gpu, size);
    cudaMalloc(&inspiring_shaw_output_gpu, size);
    cudaMalloc(&sad_poincare_output_gpu, size);

    cudaMemcpy(musing_beaver_input_gpu, musing_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(musing_beaver_input_gpu, inspiring_shaw_output_gpu, sad_poincare_output_gpu, N);

    cudaMemcpy(inspiring_shaw_output, inspiring_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(sad_poincare_output, sad_poincare_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(musing_beaver_input_gpu);
    cudaFree(inspiring_shaw_output_gpu);
    cudaFree(sad_poincare_output_gpu);
}
        ","
void func_cpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){

    for (int angry_ardinghelli = 0; angry_ardinghelli <= N; angry_ardinghelli++){

    }
        
}
        ",0.000000,0.150000,0,10,10,1,1,1,1,1
1,"


__global__ void kernel(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int angry_ardinghelli = bx*bdx + tx;
    if (angry_ardinghelli < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * musing_beaver_input_gpu;
    float * inspiring_shaw_output_gpu;
    float * sad_poincare_output_gpu;

    cudaMalloc(&musing_beaver_input_gpu, size);
    cudaMalloc(&inspiring_shaw_output_gpu, size);
    cudaMalloc(&sad_poincare_output_gpu, size);

    cudaMemcpy(musing_beaver_input_gpu, musing_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(musing_beaver_input_gpu, inspiring_shaw_output_gpu, sad_poincare_output_gpu, N);

    cudaMemcpy(inspiring_shaw_output, inspiring_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(sad_poincare_output, sad_poincare_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(musing_beaver_input_gpu);
    cudaFree(inspiring_shaw_output_gpu);
    cudaFree(sad_poincare_output_gpu);
}
        ","
void func_cpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){

    for (int angry_ardinghelli = 0; angry_ardinghelli <= N; angry_ardinghelli++){

    }
        
}
        ",0.000000,0.170000,0,100,100,1,1,1,1,1
2,"


__global__ void kernel(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int angry_ardinghelli = bx*bdx + tx;
    if (angry_ardinghelli < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * musing_beaver_input_gpu;
    float * inspiring_shaw_output_gpu;
    float * sad_poincare_output_gpu;

    cudaMalloc(&musing_beaver_input_gpu, size);
    cudaMalloc(&inspiring_shaw_output_gpu, size);
    cudaMalloc(&sad_poincare_output_gpu, size);

    cudaMemcpy(musing_beaver_input_gpu, musing_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(musing_beaver_input_gpu, inspiring_shaw_output_gpu, sad_poincare_output_gpu, N);

    cudaMemcpy(inspiring_shaw_output, inspiring_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(sad_poincare_output, sad_poincare_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(musing_beaver_input_gpu);
    cudaFree(inspiring_shaw_output_gpu);
    cudaFree(sad_poincare_output_gpu);
}
        ","
void func_cpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){

    for (int angry_ardinghelli = 0; angry_ardinghelli <= N; angry_ardinghelli++){

    }
        
}
        ",0.000000,0.160000,0,100,10,1,1,10,1,1
3,"


__global__ void kernel(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int angry_ardinghelli = bx*bdx + tx;
    if (angry_ardinghelli < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * musing_beaver_input_gpu;
    float * inspiring_shaw_output_gpu;
    float * sad_poincare_output_gpu;

    cudaMalloc(&musing_beaver_input_gpu, size);
    cudaMalloc(&inspiring_shaw_output_gpu, size);
    cudaMalloc(&sad_poincare_output_gpu, size);

    cudaMemcpy(musing_beaver_input_gpu, musing_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(musing_beaver_input_gpu, inspiring_shaw_output_gpu, sad_poincare_output_gpu, N);

    cudaMemcpy(inspiring_shaw_output, inspiring_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(sad_poincare_output, sad_poincare_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(musing_beaver_input_gpu);
    cudaFree(inspiring_shaw_output_gpu);
    cudaFree(sad_poincare_output_gpu);
}
        ","
void func_cpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){

    for (int angry_ardinghelli = 0; angry_ardinghelli <= N; angry_ardinghelli++){

    }
        
}
        ",0.000000,0.160000,0,100,1,1,1,100,1,1
4,"


__global__ void kernel(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int angry_ardinghelli = bx*bdx + tx;
    if (angry_ardinghelli < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * musing_beaver_input_gpu;
    float * inspiring_shaw_output_gpu;
    float * sad_poincare_output_gpu;

    cudaMalloc(&musing_beaver_input_gpu, size);
    cudaMalloc(&inspiring_shaw_output_gpu, size);
    cudaMalloc(&sad_poincare_output_gpu, size);

    cudaMemcpy(musing_beaver_input_gpu, musing_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(musing_beaver_input_gpu, inspiring_shaw_output_gpu, sad_poincare_output_gpu, N);

    cudaMemcpy(inspiring_shaw_output, inspiring_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(sad_poincare_output, sad_poincare_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(musing_beaver_input_gpu);
    cudaFree(inspiring_shaw_output_gpu);
    cudaFree(sad_poincare_output_gpu);
}
        ","
void func_cpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){

    for (int angry_ardinghelli = 0; angry_ardinghelli <= N; angry_ardinghelli++){

    }
        
}
        ",0.000000,0.180000,0,10000,10000,1,1,1,1,1
5,"


__global__ void kernel(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int angry_ardinghelli = bx*bdx + tx;
    if (angry_ardinghelli < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * musing_beaver_input_gpu;
    float * inspiring_shaw_output_gpu;
    float * sad_poincare_output_gpu;

    cudaMalloc(&musing_beaver_input_gpu, size);
    cudaMalloc(&inspiring_shaw_output_gpu, size);
    cudaMalloc(&sad_poincare_output_gpu, size);

    cudaMemcpy(musing_beaver_input_gpu, musing_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(musing_beaver_input_gpu, inspiring_shaw_output_gpu, sad_poincare_output_gpu, N);

    cudaMemcpy(inspiring_shaw_output, inspiring_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(sad_poincare_output, sad_poincare_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(musing_beaver_input_gpu);
    cudaFree(inspiring_shaw_output_gpu);
    cudaFree(sad_poincare_output_gpu);
}
        ","
void func_cpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){

    for (int angry_ardinghelli = 0; angry_ardinghelli <= N; angry_ardinghelli++){

    }
        
}
        ",0.000000,0.180000,0,10000,1000,1,1,10,1,1
6,"


__global__ void kernel(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int angry_ardinghelli = bx*bdx + tx;
    if (angry_ardinghelli < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * musing_beaver_input_gpu;
    float * inspiring_shaw_output_gpu;
    float * sad_poincare_output_gpu;

    cudaMalloc(&musing_beaver_input_gpu, size);
    cudaMalloc(&inspiring_shaw_output_gpu, size);
    cudaMalloc(&sad_poincare_output_gpu, size);

    cudaMemcpy(musing_beaver_input_gpu, musing_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(musing_beaver_input_gpu, inspiring_shaw_output_gpu, sad_poincare_output_gpu, N);

    cudaMemcpy(inspiring_shaw_output, inspiring_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(sad_poincare_output, sad_poincare_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(musing_beaver_input_gpu);
    cudaFree(inspiring_shaw_output_gpu);
    cudaFree(sad_poincare_output_gpu);
}
        ","
void func_cpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){

    for (int angry_ardinghelli = 0; angry_ardinghelli <= N; angry_ardinghelli++){

    }
        
}
        ",0.000000,0.160000,0,10000,100,1,1,100,1,1
7,"


__global__ void kernel(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int angry_ardinghelli = bx*bdx + tx;
    if (angry_ardinghelli < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * musing_beaver_input_gpu;
    float * inspiring_shaw_output_gpu;
    float * sad_poincare_output_gpu;

    cudaMalloc(&musing_beaver_input_gpu, size);
    cudaMalloc(&inspiring_shaw_output_gpu, size);
    cudaMalloc(&sad_poincare_output_gpu, size);

    cudaMemcpy(musing_beaver_input_gpu, musing_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(musing_beaver_input_gpu, inspiring_shaw_output_gpu, sad_poincare_output_gpu, N);

    cudaMemcpy(inspiring_shaw_output, inspiring_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(sad_poincare_output, sad_poincare_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(musing_beaver_input_gpu);
    cudaFree(inspiring_shaw_output_gpu);
    cudaFree(sad_poincare_output_gpu);
}
        ","
void func_cpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){

    for (int angry_ardinghelli = 0; angry_ardinghelli <= N; angry_ardinghelli++){

    }
        
}
        ",0.000000,0.170000,0,100000,10000,1,1,10,1,1
8,"


__global__ void kernel(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int angry_ardinghelli = bx*bdx + tx;
    if (angry_ardinghelli < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * musing_beaver_input_gpu;
    float * inspiring_shaw_output_gpu;
    float * sad_poincare_output_gpu;

    cudaMalloc(&musing_beaver_input_gpu, size);
    cudaMalloc(&inspiring_shaw_output_gpu, size);
    cudaMalloc(&sad_poincare_output_gpu, size);

    cudaMemcpy(musing_beaver_input_gpu, musing_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(musing_beaver_input_gpu, inspiring_shaw_output_gpu, sad_poincare_output_gpu, N);

    cudaMemcpy(inspiring_shaw_output, inspiring_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(sad_poincare_output, sad_poincare_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(musing_beaver_input_gpu);
    cudaFree(inspiring_shaw_output_gpu);
    cudaFree(sad_poincare_output_gpu);
}
        ","
void func_cpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){

    for (int angry_ardinghelli = 0; angry_ardinghelli <= N; angry_ardinghelli++){

    }
        
}
        ",0.000000,0.180000,0,100000,1000,1,1,100,1,1
9,"


__global__ void kernel(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int angry_ardinghelli = bx*bdx + tx;
    if (angry_ardinghelli < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * musing_beaver_input_gpu;
    float * inspiring_shaw_output_gpu;
    float * sad_poincare_output_gpu;

    cudaMalloc(&musing_beaver_input_gpu, size);
    cudaMalloc(&inspiring_shaw_output_gpu, size);
    cudaMalloc(&sad_poincare_output_gpu, size);

    cudaMemcpy(musing_beaver_input_gpu, musing_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(musing_beaver_input_gpu, inspiring_shaw_output_gpu, sad_poincare_output_gpu, N);

    cudaMemcpy(inspiring_shaw_output, inspiring_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(sad_poincare_output, sad_poincare_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(musing_beaver_input_gpu);
    cudaFree(inspiring_shaw_output_gpu);
    cudaFree(sad_poincare_output_gpu);
}
        ","
void func_cpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){

    for (int angry_ardinghelli = 0; angry_ardinghelli <= N; angry_ardinghelli++){

    }
        
}
        ",0.000000,0.160000,0,100000,100,1,1,1000,1,1
10,"


__global__ void kernel(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int angry_ardinghelli = bx*bdx + tx;
    if (angry_ardinghelli < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * musing_beaver_input_gpu;
    float * inspiring_shaw_output_gpu;
    float * sad_poincare_output_gpu;

    cudaMalloc(&musing_beaver_input_gpu, size);
    cudaMalloc(&inspiring_shaw_output_gpu, size);
    cudaMalloc(&sad_poincare_output_gpu, size);

    cudaMemcpy(musing_beaver_input_gpu, musing_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(musing_beaver_input_gpu, inspiring_shaw_output_gpu, sad_poincare_output_gpu, N);

    cudaMemcpy(inspiring_shaw_output, inspiring_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(sad_poincare_output, sad_poincare_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(musing_beaver_input_gpu);
    cudaFree(inspiring_shaw_output_gpu);
    cudaFree(sad_poincare_output_gpu);
}
        ","
void func_cpu(float * musing_beaver_input, float * inspiring_shaw_output, float * sad_poincare_output, unsigned int N){

    for (int angry_ardinghelli = 0; angry_ardinghelli <= N; angry_ardinghelli++){

    }
        
}
        ",0.000000,0.160000,0,100000,1000,1,1,100,1,1
11,"

__device__ float atomicDiv(float* address, float val) 
{ 
  int* address_as_int = (int*)address; 
  int old = *address_as_int, assumed; 
  do { 
    assumed = old; 
    old = atomicCAS(address_as_int, assumed, __float_as_int(val / __float_as_int(assumed))); 
 } while (assumed != old); return __int_as_float(old);
}


__global__ void kernel(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int heuristic_banach = bx*bdx + tx;
    if (heuristic_banach < N){
        __syncthreads();
        atomicDiv(&eloquent_lewin_output[heuristic_banach], gracious_kepler_input[heuristic_banach]);
    }
    
        
}

void func_gpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * gracious_kepler_input_gpu;
    float * eloquent_lewin_output_gpu;
    float * nostalgic_volhard_output_gpu;

    cudaMalloc(&gracious_kepler_input_gpu, size);
    cudaMalloc(&eloquent_lewin_output_gpu, size);
    cudaMalloc(&nostalgic_volhard_output_gpu, size);

    cudaMemcpy(gracious_kepler_input_gpu, gracious_kepler_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(gracious_kepler_input_gpu, eloquent_lewin_output_gpu, nostalgic_volhard_output_gpu, N);

    cudaMemcpy(eloquent_lewin_output, eloquent_lewin_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(nostalgic_volhard_output, nostalgic_volhard_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(gracious_kepler_input_gpu);
    cudaFree(eloquent_lewin_output_gpu);
    cudaFree(nostalgic_volhard_output_gpu);
}
        ","
void func_cpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){

    for (int heuristic_banach = 0; heuristic_banach <= N; heuristic_banach++){

        eloquent_lewin_output[heuristic_banach] /= gracious_kepler_input[heuristic_banach];
    }
        
}
        ",0.000000,0.240000,10,10,10,1,1,1,1,1
12,"

__device__ float atomicDiv(float* address, float val) 
{ 
  int* address_as_int = (int*)address; 
  int old = *address_as_int, assumed; 
  do { 
    assumed = old; 
    old = atomicCAS(address_as_int, assumed, __float_as_int(val / __float_as_int(assumed))); 
 } while (assumed != old); return __int_as_float(old);
}


__global__ void kernel(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int heuristic_banach = bx*bdx + tx;
    if (heuristic_banach < N){
        __syncthreads();
        atomicDiv(&eloquent_lewin_output[heuristic_banach], gracious_kepler_input[heuristic_banach]);
    }
    
        
}

void func_gpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * gracious_kepler_input_gpu;
    float * eloquent_lewin_output_gpu;
    float * nostalgic_volhard_output_gpu;

    cudaMalloc(&gracious_kepler_input_gpu, size);
    cudaMalloc(&eloquent_lewin_output_gpu, size);
    cudaMalloc(&nostalgic_volhard_output_gpu, size);

    cudaMemcpy(gracious_kepler_input_gpu, gracious_kepler_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(gracious_kepler_input_gpu, eloquent_lewin_output_gpu, nostalgic_volhard_output_gpu, N);

    cudaMemcpy(eloquent_lewin_output, eloquent_lewin_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(nostalgic_volhard_output, nostalgic_volhard_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(gracious_kepler_input_gpu);
    cudaFree(eloquent_lewin_output_gpu);
    cudaFree(nostalgic_volhard_output_gpu);
}
        ","
void func_cpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){

    for (int heuristic_banach = 0; heuristic_banach <= N; heuristic_banach++){

        eloquent_lewin_output[heuristic_banach] /= gracious_kepler_input[heuristic_banach];
    }
        
}
        ",0.000000,0.160000,100,100,100,1,1,1,1,1
13,"

__device__ float atomicDiv(float* address, float val) 
{ 
  int* address_as_int = (int*)address; 
  int old = *address_as_int, assumed; 
  do { 
    assumed = old; 
    old = atomicCAS(address_as_int, assumed, __float_as_int(val / __float_as_int(assumed))); 
 } while (assumed != old); return __int_as_float(old);
}


__global__ void kernel(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int heuristic_banach = bx*bdx + tx;
    if (heuristic_banach < N){
        __syncthreads();
        atomicDiv(&eloquent_lewin_output[heuristic_banach], gracious_kepler_input[heuristic_banach]);
    }
    
        
}

void func_gpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * gracious_kepler_input_gpu;
    float * eloquent_lewin_output_gpu;
    float * nostalgic_volhard_output_gpu;

    cudaMalloc(&gracious_kepler_input_gpu, size);
    cudaMalloc(&eloquent_lewin_output_gpu, size);
    cudaMalloc(&nostalgic_volhard_output_gpu, size);

    cudaMemcpy(gracious_kepler_input_gpu, gracious_kepler_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(gracious_kepler_input_gpu, eloquent_lewin_output_gpu, nostalgic_volhard_output_gpu, N);

    cudaMemcpy(eloquent_lewin_output, eloquent_lewin_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(nostalgic_volhard_output, nostalgic_volhard_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(gracious_kepler_input_gpu);
    cudaFree(eloquent_lewin_output_gpu);
    cudaFree(nostalgic_volhard_output_gpu);
}
        ","
void func_cpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){

    for (int heuristic_banach = 0; heuristic_banach <= N; heuristic_banach++){

        eloquent_lewin_output[heuristic_banach] /= gracious_kepler_input[heuristic_banach];
    }
        
}
        ",0.000000,0.160000,100,100,10,1,1,10,1,1
14,"

__device__ float atomicDiv(float* address, float val) 
{ 
  int* address_as_int = (int*)address; 
  int old = *address_as_int, assumed; 
  do { 
    assumed = old; 
    old = atomicCAS(address_as_int, assumed, __float_as_int(val / __float_as_int(assumed))); 
 } while (assumed != old); return __int_as_float(old);
}


__global__ void kernel(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int heuristic_banach = bx*bdx + tx;
    if (heuristic_banach < N){
        __syncthreads();
        atomicDiv(&eloquent_lewin_output[heuristic_banach], gracious_kepler_input[heuristic_banach]);
    }
    
        
}

void func_gpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * gracious_kepler_input_gpu;
    float * eloquent_lewin_output_gpu;
    float * nostalgic_volhard_output_gpu;

    cudaMalloc(&gracious_kepler_input_gpu, size);
    cudaMalloc(&eloquent_lewin_output_gpu, size);
    cudaMalloc(&nostalgic_volhard_output_gpu, size);

    cudaMemcpy(gracious_kepler_input_gpu, gracious_kepler_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(gracious_kepler_input_gpu, eloquent_lewin_output_gpu, nostalgic_volhard_output_gpu, N);

    cudaMemcpy(eloquent_lewin_output, eloquent_lewin_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(nostalgic_volhard_output, nostalgic_volhard_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(gracious_kepler_input_gpu);
    cudaFree(eloquent_lewin_output_gpu);
    cudaFree(nostalgic_volhard_output_gpu);
}
        ","
void func_cpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){

    for (int heuristic_banach = 0; heuristic_banach <= N; heuristic_banach++){

        eloquent_lewin_output[heuristic_banach] /= gracious_kepler_input[heuristic_banach];
    }
        
}
        ",0.000000,0.170000,100,100,1,1,1,100,1,1
15,"

__device__ float atomicDiv(float* address, float val) 
{ 
  int* address_as_int = (int*)address; 
  int old = *address_as_int, assumed; 
  do { 
    assumed = old; 
    old = atomicCAS(address_as_int, assumed, __float_as_int(val / __float_as_int(assumed))); 
 } while (assumed != old); return __int_as_float(old);
}


__global__ void kernel(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int heuristic_banach = bx*bdx + tx;
    if (heuristic_banach < N){
        __syncthreads();
        atomicDiv(&eloquent_lewin_output[heuristic_banach], gracious_kepler_input[heuristic_banach]);
    }
    
        
}

void func_gpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * gracious_kepler_input_gpu;
    float * eloquent_lewin_output_gpu;
    float * nostalgic_volhard_output_gpu;

    cudaMalloc(&gracious_kepler_input_gpu, size);
    cudaMalloc(&eloquent_lewin_output_gpu, size);
    cudaMalloc(&nostalgic_volhard_output_gpu, size);

    cudaMemcpy(gracious_kepler_input_gpu, gracious_kepler_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(gracious_kepler_input_gpu, eloquent_lewin_output_gpu, nostalgic_volhard_output_gpu, N);

    cudaMemcpy(eloquent_lewin_output, eloquent_lewin_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(nostalgic_volhard_output, nostalgic_volhard_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(gracious_kepler_input_gpu);
    cudaFree(eloquent_lewin_output_gpu);
    cudaFree(nostalgic_volhard_output_gpu);
}
        ","
void func_cpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){

    for (int heuristic_banach = 0; heuristic_banach <= N; heuristic_banach++){

        eloquent_lewin_output[heuristic_banach] /= gracious_kepler_input[heuristic_banach];
    }
        
}
        ",0.000000,0.170000,10000,10000,10000,1,1,1,1,1
16,"

__device__ float atomicDiv(float* address, float val) 
{ 
  int* address_as_int = (int*)address; 
  int old = *address_as_int, assumed; 
  do { 
    assumed = old; 
    old = atomicCAS(address_as_int, assumed, __float_as_int(val / __float_as_int(assumed))); 
 } while (assumed != old); return __int_as_float(old);
}


__global__ void kernel(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int heuristic_banach = bx*bdx + tx;
    if (heuristic_banach < N){
        __syncthreads();
        atomicDiv(&eloquent_lewin_output[heuristic_banach], gracious_kepler_input[heuristic_banach]);
    }
    
        
}

void func_gpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * gracious_kepler_input_gpu;
    float * eloquent_lewin_output_gpu;
    float * nostalgic_volhard_output_gpu;

    cudaMalloc(&gracious_kepler_input_gpu, size);
    cudaMalloc(&eloquent_lewin_output_gpu, size);
    cudaMalloc(&nostalgic_volhard_output_gpu, size);

    cudaMemcpy(gracious_kepler_input_gpu, gracious_kepler_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(gracious_kepler_input_gpu, eloquent_lewin_output_gpu, nostalgic_volhard_output_gpu, N);

    cudaMemcpy(eloquent_lewin_output, eloquent_lewin_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(nostalgic_volhard_output, nostalgic_volhard_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(gracious_kepler_input_gpu);
    cudaFree(eloquent_lewin_output_gpu);
    cudaFree(nostalgic_volhard_output_gpu);
}
        ","
void func_cpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){

    for (int heuristic_banach = 0; heuristic_banach <= N; heuristic_banach++){

        eloquent_lewin_output[heuristic_banach] /= gracious_kepler_input[heuristic_banach];
    }
        
}
        ",0.000000,0.160000,10000,10000,1000,1,1,10,1,1
17,"

__device__ float atomicDiv(float* address, float val) 
{ 
  int* address_as_int = (int*)address; 
  int old = *address_as_int, assumed; 
  do { 
    assumed = old; 
    old = atomicCAS(address_as_int, assumed, __float_as_int(val / __float_as_int(assumed))); 
 } while (assumed != old); return __int_as_float(old);
}


__global__ void kernel(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int heuristic_banach = bx*bdx + tx;
    if (heuristic_banach < N){
        __syncthreads();
        atomicDiv(&eloquent_lewin_output[heuristic_banach], gracious_kepler_input[heuristic_banach]);
    }
    
        
}

void func_gpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * gracious_kepler_input_gpu;
    float * eloquent_lewin_output_gpu;
    float * nostalgic_volhard_output_gpu;

    cudaMalloc(&gracious_kepler_input_gpu, size);
    cudaMalloc(&eloquent_lewin_output_gpu, size);
    cudaMalloc(&nostalgic_volhard_output_gpu, size);

    cudaMemcpy(gracious_kepler_input_gpu, gracious_kepler_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(gracious_kepler_input_gpu, eloquent_lewin_output_gpu, nostalgic_volhard_output_gpu, N);

    cudaMemcpy(eloquent_lewin_output, eloquent_lewin_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(nostalgic_volhard_output, nostalgic_volhard_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(gracious_kepler_input_gpu);
    cudaFree(eloquent_lewin_output_gpu);
    cudaFree(nostalgic_volhard_output_gpu);
}
        ","
void func_cpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){

    for (int heuristic_banach = 0; heuristic_banach <= N; heuristic_banach++){

        eloquent_lewin_output[heuristic_banach] /= gracious_kepler_input[heuristic_banach];
    }
        
}
        ",0.000000,0.180000,10000,10000,100,1,1,100,1,1
18,"

__device__ float atomicDiv(float* address, float val) 
{ 
  int* address_as_int = (int*)address; 
  int old = *address_as_int, assumed; 
  do { 
    assumed = old; 
    old = atomicCAS(address_as_int, assumed, __float_as_int(val / __float_as_int(assumed))); 
 } while (assumed != old); return __int_as_float(old);
}


__global__ void kernel(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int heuristic_banach = bx*bdx + tx;
    if (heuristic_banach < N){
        __syncthreads();
        atomicDiv(&eloquent_lewin_output[heuristic_banach], gracious_kepler_input[heuristic_banach]);
    }
    
        
}

void func_gpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * gracious_kepler_input_gpu;
    float * eloquent_lewin_output_gpu;
    float * nostalgic_volhard_output_gpu;

    cudaMalloc(&gracious_kepler_input_gpu, size);
    cudaMalloc(&eloquent_lewin_output_gpu, size);
    cudaMalloc(&nostalgic_volhard_output_gpu, size);

    cudaMemcpy(gracious_kepler_input_gpu, gracious_kepler_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(gracious_kepler_input_gpu, eloquent_lewin_output_gpu, nostalgic_volhard_output_gpu, N);

    cudaMemcpy(eloquent_lewin_output, eloquent_lewin_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(nostalgic_volhard_output, nostalgic_volhard_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(gracious_kepler_input_gpu);
    cudaFree(eloquent_lewin_output_gpu);
    cudaFree(nostalgic_volhard_output_gpu);
}
        ","
void func_cpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){

    for (int heuristic_banach = 0; heuristic_banach <= N; heuristic_banach++){

        eloquent_lewin_output[heuristic_banach] /= gracious_kepler_input[heuristic_banach];
    }
        
}
        ",0.000000,0.180000,100000,100000,10000,1,1,10,1,1
19,"

__device__ float atomicDiv(float* address, float val) 
{ 
  int* address_as_int = (int*)address; 
  int old = *address_as_int, assumed; 
  do { 
    assumed = old; 
    old = atomicCAS(address_as_int, assumed, __float_as_int(val / __float_as_int(assumed))); 
 } while (assumed != old); return __int_as_float(old);
}


__global__ void kernel(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int heuristic_banach = bx*bdx + tx;
    if (heuristic_banach < N){
        __syncthreads();
        atomicDiv(&eloquent_lewin_output[heuristic_banach], gracious_kepler_input[heuristic_banach]);
    }
    
        
}

void func_gpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * gracious_kepler_input_gpu;
    float * eloquent_lewin_output_gpu;
    float * nostalgic_volhard_output_gpu;

    cudaMalloc(&gracious_kepler_input_gpu, size);
    cudaMalloc(&eloquent_lewin_output_gpu, size);
    cudaMalloc(&nostalgic_volhard_output_gpu, size);

    cudaMemcpy(gracious_kepler_input_gpu, gracious_kepler_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(gracious_kepler_input_gpu, eloquent_lewin_output_gpu, nostalgic_volhard_output_gpu, N);

    cudaMemcpy(eloquent_lewin_output, eloquent_lewin_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(nostalgic_volhard_output, nostalgic_volhard_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(gracious_kepler_input_gpu);
    cudaFree(eloquent_lewin_output_gpu);
    cudaFree(nostalgic_volhard_output_gpu);
}
        ","
void func_cpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){

    for (int heuristic_banach = 0; heuristic_banach <= N; heuristic_banach++){

        eloquent_lewin_output[heuristic_banach] /= gracious_kepler_input[heuristic_banach];
    }
        
}
        ",0.000000,0.170000,100000,100000,1000,1,1,100,1,1
20,"

__device__ float atomicDiv(float* address, float val) 
{ 
  int* address_as_int = (int*)address; 
  int old = *address_as_int, assumed; 
  do { 
    assumed = old; 
    old = atomicCAS(address_as_int, assumed, __float_as_int(val / __float_as_int(assumed))); 
 } while (assumed != old); return __int_as_float(old);
}


__global__ void kernel(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int heuristic_banach = bx*bdx + tx;
    if (heuristic_banach < N){
        __syncthreads();
        atomicDiv(&eloquent_lewin_output[heuristic_banach], gracious_kepler_input[heuristic_banach]);
    }
    
        
}

void func_gpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * gracious_kepler_input_gpu;
    float * eloquent_lewin_output_gpu;
    float * nostalgic_volhard_output_gpu;

    cudaMalloc(&gracious_kepler_input_gpu, size);
    cudaMalloc(&eloquent_lewin_output_gpu, size);
    cudaMalloc(&nostalgic_volhard_output_gpu, size);

    cudaMemcpy(gracious_kepler_input_gpu, gracious_kepler_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(gracious_kepler_input_gpu, eloquent_lewin_output_gpu, nostalgic_volhard_output_gpu, N);

    cudaMemcpy(eloquent_lewin_output, eloquent_lewin_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(nostalgic_volhard_output, nostalgic_volhard_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(gracious_kepler_input_gpu);
    cudaFree(eloquent_lewin_output_gpu);
    cudaFree(nostalgic_volhard_output_gpu);
}
        ","
void func_cpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){

    for (int heuristic_banach = 0; heuristic_banach <= N; heuristic_banach++){

        eloquent_lewin_output[heuristic_banach] /= gracious_kepler_input[heuristic_banach];
    }
        
}
        ",0.000000,0.170000,100000,100000,100,1,1,1000,1,1
21,"

__device__ float atomicDiv(float* address, float val) 
{ 
  int* address_as_int = (int*)address; 
  int old = *address_as_int, assumed; 
  do { 
    assumed = old; 
    old = atomicCAS(address_as_int, assumed, __float_as_int(val / __float_as_int(assumed))); 
 } while (assumed != old); return __int_as_float(old);
}


__global__ void kernel(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int heuristic_banach = bx*bdx + tx;
    if (heuristic_banach < N){
        __syncthreads();
        atomicDiv(&eloquent_lewin_output[heuristic_banach], gracious_kepler_input[heuristic_banach]);
    }
    
        
}

void func_gpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * gracious_kepler_input_gpu;
    float * eloquent_lewin_output_gpu;
    float * nostalgic_volhard_output_gpu;

    cudaMalloc(&gracious_kepler_input_gpu, size);
    cudaMalloc(&eloquent_lewin_output_gpu, size);
    cudaMalloc(&nostalgic_volhard_output_gpu, size);

    cudaMemcpy(gracious_kepler_input_gpu, gracious_kepler_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(gracious_kepler_input_gpu, eloquent_lewin_output_gpu, nostalgic_volhard_output_gpu, N);

    cudaMemcpy(eloquent_lewin_output, eloquent_lewin_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(nostalgic_volhard_output, nostalgic_volhard_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(gracious_kepler_input_gpu);
    cudaFree(eloquent_lewin_output_gpu);
    cudaFree(nostalgic_volhard_output_gpu);
}
        ","
void func_cpu(float * gracious_kepler_input, float * eloquent_lewin_output, float * nostalgic_volhard_output, unsigned int N){

    for (int heuristic_banach = 0; heuristic_banach <= N; heuristic_banach++){

        eloquent_lewin_output[heuristic_banach] /= gracious_kepler_input[heuristic_banach];
    }
        
}
        ",0.000000,0.180000,100000,100000,1000,1,1,100,1,1
22,"


__global__ void kernel(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int unruffled_northcutt = bx*bdx + tx;
    if (unruffled_northcutt < N){
        __syncthreads();
        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
    
        
}

void func_gpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * sad_shirley_input_gpu;
    float * serene_panini_input_gpu;
    float * vigilant_fermi_output_gpu;
    float * confident_dijkstra_output_gpu;

    cudaMalloc(&sad_shirley_input_gpu, size);
    cudaMalloc(&serene_panini_input_gpu, size);
    cudaMalloc(&vigilant_fermi_output_gpu, size);
    cudaMalloc(&confident_dijkstra_output_gpu, size);

    cudaMemcpy(sad_shirley_input_gpu, sad_shirley_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(serene_panini_input_gpu, serene_panini_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(sad_shirley_input_gpu, serene_panini_input_gpu, vigilant_fermi_output_gpu, confident_dijkstra_output_gpu, N);

    cudaMemcpy(vigilant_fermi_output, vigilant_fermi_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(confident_dijkstra_output, confident_dijkstra_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(sad_shirley_input_gpu);
    cudaFree(serene_panini_input_gpu);
    cudaFree(vigilant_fermi_output_gpu);
    cudaFree(confident_dijkstra_output_gpu);
}
        ","
void func_cpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){

    for (int unruffled_northcutt = 0; unruffled_northcutt <= N; unruffled_northcutt++){

        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
        
}
        ",0.000000,0.210000,0,10,10,1,1,1,1,1
23,"


__global__ void kernel(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int unruffled_northcutt = bx*bdx + tx;
    if (unruffled_northcutt < N){
        __syncthreads();
        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
    
        
}

void func_gpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * sad_shirley_input_gpu;
    float * serene_panini_input_gpu;
    float * vigilant_fermi_output_gpu;
    float * confident_dijkstra_output_gpu;

    cudaMalloc(&sad_shirley_input_gpu, size);
    cudaMalloc(&serene_panini_input_gpu, size);
    cudaMalloc(&vigilant_fermi_output_gpu, size);
    cudaMalloc(&confident_dijkstra_output_gpu, size);

    cudaMemcpy(sad_shirley_input_gpu, sad_shirley_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(serene_panini_input_gpu, serene_panini_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(sad_shirley_input_gpu, serene_panini_input_gpu, vigilant_fermi_output_gpu, confident_dijkstra_output_gpu, N);

    cudaMemcpy(vigilant_fermi_output, vigilant_fermi_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(confident_dijkstra_output, confident_dijkstra_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(sad_shirley_input_gpu);
    cudaFree(serene_panini_input_gpu);
    cudaFree(vigilant_fermi_output_gpu);
    cudaFree(confident_dijkstra_output_gpu);
}
        ","
void func_cpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){

    for (int unruffled_northcutt = 0; unruffled_northcutt <= N; unruffled_northcutt++){

        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
        
}
        ",0.000000,0.160000,0,100,100,1,1,1,1,1
24,"


__global__ void kernel(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int unruffled_northcutt = bx*bdx + tx;
    if (unruffled_northcutt < N){
        __syncthreads();
        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
    
        
}

void func_gpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * sad_shirley_input_gpu;
    float * serene_panini_input_gpu;
    float * vigilant_fermi_output_gpu;
    float * confident_dijkstra_output_gpu;

    cudaMalloc(&sad_shirley_input_gpu, size);
    cudaMalloc(&serene_panini_input_gpu, size);
    cudaMalloc(&vigilant_fermi_output_gpu, size);
    cudaMalloc(&confident_dijkstra_output_gpu, size);

    cudaMemcpy(sad_shirley_input_gpu, sad_shirley_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(serene_panini_input_gpu, serene_panini_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(sad_shirley_input_gpu, serene_panini_input_gpu, vigilant_fermi_output_gpu, confident_dijkstra_output_gpu, N);

    cudaMemcpy(vigilant_fermi_output, vigilant_fermi_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(confident_dijkstra_output, confident_dijkstra_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(sad_shirley_input_gpu);
    cudaFree(serene_panini_input_gpu);
    cudaFree(vigilant_fermi_output_gpu);
    cudaFree(confident_dijkstra_output_gpu);
}
        ","
void func_cpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){

    for (int unruffled_northcutt = 0; unruffled_northcutt <= N; unruffled_northcutt++){

        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
        
}
        ",0.000000,0.160000,0,100,10,1,1,10,1,1
25,"


__global__ void kernel(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int unruffled_northcutt = bx*bdx + tx;
    if (unruffled_northcutt < N){
        __syncthreads();
        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
    
        
}

void func_gpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * sad_shirley_input_gpu;
    float * serene_panini_input_gpu;
    float * vigilant_fermi_output_gpu;
    float * confident_dijkstra_output_gpu;

    cudaMalloc(&sad_shirley_input_gpu, size);
    cudaMalloc(&serene_panini_input_gpu, size);
    cudaMalloc(&vigilant_fermi_output_gpu, size);
    cudaMalloc(&confident_dijkstra_output_gpu, size);

    cudaMemcpy(sad_shirley_input_gpu, sad_shirley_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(serene_panini_input_gpu, serene_panini_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(sad_shirley_input_gpu, serene_panini_input_gpu, vigilant_fermi_output_gpu, confident_dijkstra_output_gpu, N);

    cudaMemcpy(vigilant_fermi_output, vigilant_fermi_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(confident_dijkstra_output, confident_dijkstra_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(sad_shirley_input_gpu);
    cudaFree(serene_panini_input_gpu);
    cudaFree(vigilant_fermi_output_gpu);
    cudaFree(confident_dijkstra_output_gpu);
}
        ","
void func_cpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){

    for (int unruffled_northcutt = 0; unruffled_northcutt <= N; unruffled_northcutt++){

        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
        
}
        ",0.000000,0.150000,0,100,1,1,1,100,1,1
26,"


__global__ void kernel(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int unruffled_northcutt = bx*bdx + tx;
    if (unruffled_northcutt < N){
        __syncthreads();
        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
    
        
}

void func_gpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * sad_shirley_input_gpu;
    float * serene_panini_input_gpu;
    float * vigilant_fermi_output_gpu;
    float * confident_dijkstra_output_gpu;

    cudaMalloc(&sad_shirley_input_gpu, size);
    cudaMalloc(&serene_panini_input_gpu, size);
    cudaMalloc(&vigilant_fermi_output_gpu, size);
    cudaMalloc(&confident_dijkstra_output_gpu, size);

    cudaMemcpy(sad_shirley_input_gpu, sad_shirley_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(serene_panini_input_gpu, serene_panini_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(sad_shirley_input_gpu, serene_panini_input_gpu, vigilant_fermi_output_gpu, confident_dijkstra_output_gpu, N);

    cudaMemcpy(vigilant_fermi_output, vigilant_fermi_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(confident_dijkstra_output, confident_dijkstra_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(sad_shirley_input_gpu);
    cudaFree(serene_panini_input_gpu);
    cudaFree(vigilant_fermi_output_gpu);
    cudaFree(confident_dijkstra_output_gpu);
}
        ","
void func_cpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){

    for (int unruffled_northcutt = 0; unruffled_northcutt <= N; unruffled_northcutt++){

        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
        
}
        ",0.000000,0.160000,0,10000,10000,1,1,1,1,1
27,"


__global__ void kernel(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int unruffled_northcutt = bx*bdx + tx;
    if (unruffled_northcutt < N){
        __syncthreads();
        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
    
        
}

void func_gpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * sad_shirley_input_gpu;
    float * serene_panini_input_gpu;
    float * vigilant_fermi_output_gpu;
    float * confident_dijkstra_output_gpu;

    cudaMalloc(&sad_shirley_input_gpu, size);
    cudaMalloc(&serene_panini_input_gpu, size);
    cudaMalloc(&vigilant_fermi_output_gpu, size);
    cudaMalloc(&confident_dijkstra_output_gpu, size);

    cudaMemcpy(sad_shirley_input_gpu, sad_shirley_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(serene_panini_input_gpu, serene_panini_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(sad_shirley_input_gpu, serene_panini_input_gpu, vigilant_fermi_output_gpu, confident_dijkstra_output_gpu, N);

    cudaMemcpy(vigilant_fermi_output, vigilant_fermi_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(confident_dijkstra_output, confident_dijkstra_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(sad_shirley_input_gpu);
    cudaFree(serene_panini_input_gpu);
    cudaFree(vigilant_fermi_output_gpu);
    cudaFree(confident_dijkstra_output_gpu);
}
        ","
void func_cpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){

    for (int unruffled_northcutt = 0; unruffled_northcutt <= N; unruffled_northcutt++){

        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
        
}
        ",0.000000,0.170000,0,10000,1000,1,1,10,1,1
28,"


__global__ void kernel(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int unruffled_northcutt = bx*bdx + tx;
    if (unruffled_northcutt < N){
        __syncthreads();
        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
    
        
}

void func_gpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * sad_shirley_input_gpu;
    float * serene_panini_input_gpu;
    float * vigilant_fermi_output_gpu;
    float * confident_dijkstra_output_gpu;

    cudaMalloc(&sad_shirley_input_gpu, size);
    cudaMalloc(&serene_panini_input_gpu, size);
    cudaMalloc(&vigilant_fermi_output_gpu, size);
    cudaMalloc(&confident_dijkstra_output_gpu, size);

    cudaMemcpy(sad_shirley_input_gpu, sad_shirley_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(serene_panini_input_gpu, serene_panini_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(sad_shirley_input_gpu, serene_panini_input_gpu, vigilant_fermi_output_gpu, confident_dijkstra_output_gpu, N);

    cudaMemcpy(vigilant_fermi_output, vigilant_fermi_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(confident_dijkstra_output, confident_dijkstra_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(sad_shirley_input_gpu);
    cudaFree(serene_panini_input_gpu);
    cudaFree(vigilant_fermi_output_gpu);
    cudaFree(confident_dijkstra_output_gpu);
}
        ","
void func_cpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){

    for (int unruffled_northcutt = 0; unruffled_northcutt <= N; unruffled_northcutt++){

        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
        
}
        ",0.000000,0.180000,0,10000,100,1,1,100,1,1
29,"


__global__ void kernel(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int unruffled_northcutt = bx*bdx + tx;
    if (unruffled_northcutt < N){
        __syncthreads();
        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
    
        
}

void func_gpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * sad_shirley_input_gpu;
    float * serene_panini_input_gpu;
    float * vigilant_fermi_output_gpu;
    float * confident_dijkstra_output_gpu;

    cudaMalloc(&sad_shirley_input_gpu, size);
    cudaMalloc(&serene_panini_input_gpu, size);
    cudaMalloc(&vigilant_fermi_output_gpu, size);
    cudaMalloc(&confident_dijkstra_output_gpu, size);

    cudaMemcpy(sad_shirley_input_gpu, sad_shirley_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(serene_panini_input_gpu, serene_panini_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(sad_shirley_input_gpu, serene_panini_input_gpu, vigilant_fermi_output_gpu, confident_dijkstra_output_gpu, N);

    cudaMemcpy(vigilant_fermi_output, vigilant_fermi_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(confident_dijkstra_output, confident_dijkstra_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(sad_shirley_input_gpu);
    cudaFree(serene_panini_input_gpu);
    cudaFree(vigilant_fermi_output_gpu);
    cudaFree(confident_dijkstra_output_gpu);
}
        ","
void func_cpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){

    for (int unruffled_northcutt = 0; unruffled_northcutt <= N; unruffled_northcutt++){

        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
        
}
        ",0.000000,0.160000,0,100000,10000,1,1,10,1,1
30,"


__global__ void kernel(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int unruffled_northcutt = bx*bdx + tx;
    if (unruffled_northcutt < N){
        __syncthreads();
        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
    
        
}

void func_gpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * sad_shirley_input_gpu;
    float * serene_panini_input_gpu;
    float * vigilant_fermi_output_gpu;
    float * confident_dijkstra_output_gpu;

    cudaMalloc(&sad_shirley_input_gpu, size);
    cudaMalloc(&serene_panini_input_gpu, size);
    cudaMalloc(&vigilant_fermi_output_gpu, size);
    cudaMalloc(&confident_dijkstra_output_gpu, size);

    cudaMemcpy(sad_shirley_input_gpu, sad_shirley_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(serene_panini_input_gpu, serene_panini_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(sad_shirley_input_gpu, serene_panini_input_gpu, vigilant_fermi_output_gpu, confident_dijkstra_output_gpu, N);

    cudaMemcpy(vigilant_fermi_output, vigilant_fermi_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(confident_dijkstra_output, confident_dijkstra_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(sad_shirley_input_gpu);
    cudaFree(serene_panini_input_gpu);
    cudaFree(vigilant_fermi_output_gpu);
    cudaFree(confident_dijkstra_output_gpu);
}
        ","
void func_cpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){

    for (int unruffled_northcutt = 0; unruffled_northcutt <= N; unruffled_northcutt++){

        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
        
}
        ",0.000000,0.180000,0,100000,1000,1,1,100,1,1
31,"


__global__ void kernel(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int unruffled_northcutt = bx*bdx + tx;
    if (unruffled_northcutt < N){
        __syncthreads();
        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
    
        
}

void func_gpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * sad_shirley_input_gpu;
    float * serene_panini_input_gpu;
    float * vigilant_fermi_output_gpu;
    float * confident_dijkstra_output_gpu;

    cudaMalloc(&sad_shirley_input_gpu, size);
    cudaMalloc(&serene_panini_input_gpu, size);
    cudaMalloc(&vigilant_fermi_output_gpu, size);
    cudaMalloc(&confident_dijkstra_output_gpu, size);

    cudaMemcpy(sad_shirley_input_gpu, sad_shirley_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(serene_panini_input_gpu, serene_panini_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(sad_shirley_input_gpu, serene_panini_input_gpu, vigilant_fermi_output_gpu, confident_dijkstra_output_gpu, N);

    cudaMemcpy(vigilant_fermi_output, vigilant_fermi_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(confident_dijkstra_output, confident_dijkstra_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(sad_shirley_input_gpu);
    cudaFree(serene_panini_input_gpu);
    cudaFree(vigilant_fermi_output_gpu);
    cudaFree(confident_dijkstra_output_gpu);
}
        ","
void func_cpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){

    for (int unruffled_northcutt = 0; unruffled_northcutt <= N; unruffled_northcutt++){

        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
        
}
        ",0.000000,0.160000,0,100000,100,1,1,1000,1,1
32,"


__global__ void kernel(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int unruffled_northcutt = bx*bdx + tx;
    if (unruffled_northcutt < N){
        __syncthreads();
        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
    
        
}

void func_gpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * sad_shirley_input_gpu;
    float * serene_panini_input_gpu;
    float * vigilant_fermi_output_gpu;
    float * confident_dijkstra_output_gpu;

    cudaMalloc(&sad_shirley_input_gpu, size);
    cudaMalloc(&serene_panini_input_gpu, size);
    cudaMalloc(&vigilant_fermi_output_gpu, size);
    cudaMalloc(&confident_dijkstra_output_gpu, size);

    cudaMemcpy(sad_shirley_input_gpu, sad_shirley_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(serene_panini_input_gpu, serene_panini_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(sad_shirley_input_gpu, serene_panini_input_gpu, vigilant_fermi_output_gpu, confident_dijkstra_output_gpu, N);

    cudaMemcpy(vigilant_fermi_output, vigilant_fermi_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(confident_dijkstra_output, confident_dijkstra_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(sad_shirley_input_gpu);
    cudaFree(serene_panini_input_gpu);
    cudaFree(vigilant_fermi_output_gpu);
    cudaFree(confident_dijkstra_output_gpu);
}
        ","
void func_cpu(float * sad_shirley_input, float * serene_panini_input, float * vigilant_fermi_output, float * confident_dijkstra_output, unsigned int N){

    for (int unruffled_northcutt = 0; unruffled_northcutt <= N; unruffled_northcutt++){

        confident_dijkstra_output[unruffled_northcutt] /= serene_panini_input[unruffled_northcutt];
    }
        
}
        ",0.000000,0.160000,0,100000,1000,1,1,100,1,1
33,"


__global__ void kernel(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int elegant_ardinghelli = bx*bdx + tx;
    if (elegant_ardinghelli < N){
        atomicAdd(&awesome_brattain_output[elegant_ardinghelli], confident_mahavira_input[elegant_ardinghelli]);
    }
    
        
}

void func_gpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * confident_mahavira_input_gpu;
    float * awesome_brattain_output_gpu;
    float * agitated_murdock_output_gpu;

    cudaMalloc(&confident_mahavira_input_gpu, size);
    cudaMalloc(&awesome_brattain_output_gpu, size);
    cudaMalloc(&agitated_murdock_output_gpu, size);

    cudaMemcpy(confident_mahavira_input_gpu, confident_mahavira_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(confident_mahavira_input_gpu, awesome_brattain_output_gpu, agitated_murdock_output_gpu, N);

    cudaMemcpy(awesome_brattain_output, awesome_brattain_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_murdock_output, agitated_murdock_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(confident_mahavira_input_gpu);
    cudaFree(awesome_brattain_output_gpu);
    cudaFree(agitated_murdock_output_gpu);
}
        ","
void func_cpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){

    for (int elegant_ardinghelli = 0; elegant_ardinghelli <= N; elegant_ardinghelli++){
        awesome_brattain_output[elegant_ardinghelli] += confident_mahavira_input[elegant_ardinghelli];
    }
        
}
        ",0.000000,0.210000,0,10,10,1,1,1,1,1
34,"


__global__ void kernel(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int elegant_ardinghelli = bx*bdx + tx;
    if (elegant_ardinghelli < N){
        atomicAdd(&awesome_brattain_output[elegant_ardinghelli], confident_mahavira_input[elegant_ardinghelli]);
    }
    
        
}

void func_gpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * confident_mahavira_input_gpu;
    float * awesome_brattain_output_gpu;
    float * agitated_murdock_output_gpu;

    cudaMalloc(&confident_mahavira_input_gpu, size);
    cudaMalloc(&awesome_brattain_output_gpu, size);
    cudaMalloc(&agitated_murdock_output_gpu, size);

    cudaMemcpy(confident_mahavira_input_gpu, confident_mahavira_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(confident_mahavira_input_gpu, awesome_brattain_output_gpu, agitated_murdock_output_gpu, N);

    cudaMemcpy(awesome_brattain_output, awesome_brattain_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_murdock_output, agitated_murdock_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(confident_mahavira_input_gpu);
    cudaFree(awesome_brattain_output_gpu);
    cudaFree(agitated_murdock_output_gpu);
}
        ","
void func_cpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){

    for (int elegant_ardinghelli = 0; elegant_ardinghelli <= N; elegant_ardinghelli++){
        awesome_brattain_output[elegant_ardinghelli] += confident_mahavira_input[elegant_ardinghelli];
    }
        
}
        ",0.000000,0.160000,0,100,100,1,1,1,1,1
35,"


__global__ void kernel(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int elegant_ardinghelli = bx*bdx + tx;
    if (elegant_ardinghelli < N){
        atomicAdd(&awesome_brattain_output[elegant_ardinghelli], confident_mahavira_input[elegant_ardinghelli]);
    }
    
        
}

void func_gpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * confident_mahavira_input_gpu;
    float * awesome_brattain_output_gpu;
    float * agitated_murdock_output_gpu;

    cudaMalloc(&confident_mahavira_input_gpu, size);
    cudaMalloc(&awesome_brattain_output_gpu, size);
    cudaMalloc(&agitated_murdock_output_gpu, size);

    cudaMemcpy(confident_mahavira_input_gpu, confident_mahavira_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(confident_mahavira_input_gpu, awesome_brattain_output_gpu, agitated_murdock_output_gpu, N);

    cudaMemcpy(awesome_brattain_output, awesome_brattain_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_murdock_output, agitated_murdock_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(confident_mahavira_input_gpu);
    cudaFree(awesome_brattain_output_gpu);
    cudaFree(agitated_murdock_output_gpu);
}
        ","
void func_cpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){

    for (int elegant_ardinghelli = 0; elegant_ardinghelli <= N; elegant_ardinghelli++){
        awesome_brattain_output[elegant_ardinghelli] += confident_mahavira_input[elegant_ardinghelli];
    }
        
}
        ",0.000000,0.150000,0,100,10,1,1,10,1,1
36,"


__global__ void kernel(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int elegant_ardinghelli = bx*bdx + tx;
    if (elegant_ardinghelli < N){
        atomicAdd(&awesome_brattain_output[elegant_ardinghelli], confident_mahavira_input[elegant_ardinghelli]);
    }
    
        
}

void func_gpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * confident_mahavira_input_gpu;
    float * awesome_brattain_output_gpu;
    float * agitated_murdock_output_gpu;

    cudaMalloc(&confident_mahavira_input_gpu, size);
    cudaMalloc(&awesome_brattain_output_gpu, size);
    cudaMalloc(&agitated_murdock_output_gpu, size);

    cudaMemcpy(confident_mahavira_input_gpu, confident_mahavira_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(confident_mahavira_input_gpu, awesome_brattain_output_gpu, agitated_murdock_output_gpu, N);

    cudaMemcpy(awesome_brattain_output, awesome_brattain_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_murdock_output, agitated_murdock_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(confident_mahavira_input_gpu);
    cudaFree(awesome_brattain_output_gpu);
    cudaFree(agitated_murdock_output_gpu);
}
        ","
void func_cpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){

    for (int elegant_ardinghelli = 0; elegant_ardinghelli <= N; elegant_ardinghelli++){
        awesome_brattain_output[elegant_ardinghelli] += confident_mahavira_input[elegant_ardinghelli];
    }
        
}
        ",0.000000,0.160000,0,100,1,1,1,100,1,1
37,"


__global__ void kernel(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int elegant_ardinghelli = bx*bdx + tx;
    if (elegant_ardinghelli < N){
        atomicAdd(&awesome_brattain_output[elegant_ardinghelli], confident_mahavira_input[elegant_ardinghelli]);
    }
    
        
}

void func_gpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * confident_mahavira_input_gpu;
    float * awesome_brattain_output_gpu;
    float * agitated_murdock_output_gpu;

    cudaMalloc(&confident_mahavira_input_gpu, size);
    cudaMalloc(&awesome_brattain_output_gpu, size);
    cudaMalloc(&agitated_murdock_output_gpu, size);

    cudaMemcpy(confident_mahavira_input_gpu, confident_mahavira_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(confident_mahavira_input_gpu, awesome_brattain_output_gpu, agitated_murdock_output_gpu, N);

    cudaMemcpy(awesome_brattain_output, awesome_brattain_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_murdock_output, agitated_murdock_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(confident_mahavira_input_gpu);
    cudaFree(awesome_brattain_output_gpu);
    cudaFree(agitated_murdock_output_gpu);
}
        ","
void func_cpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){

    for (int elegant_ardinghelli = 0; elegant_ardinghelli <= N; elegant_ardinghelli++){
        awesome_brattain_output[elegant_ardinghelli] += confident_mahavira_input[elegant_ardinghelli];
    }
        
}
        ",0.000000,0.160000,0,10000,10000,1,1,1,1,1
38,"


__global__ void kernel(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int elegant_ardinghelli = bx*bdx + tx;
    if (elegant_ardinghelli < N){
        atomicAdd(&awesome_brattain_output[elegant_ardinghelli], confident_mahavira_input[elegant_ardinghelli]);
    }
    
        
}

void func_gpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * confident_mahavira_input_gpu;
    float * awesome_brattain_output_gpu;
    float * agitated_murdock_output_gpu;

    cudaMalloc(&confident_mahavira_input_gpu, size);
    cudaMalloc(&awesome_brattain_output_gpu, size);
    cudaMalloc(&agitated_murdock_output_gpu, size);

    cudaMemcpy(confident_mahavira_input_gpu, confident_mahavira_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(confident_mahavira_input_gpu, awesome_brattain_output_gpu, agitated_murdock_output_gpu, N);

    cudaMemcpy(awesome_brattain_output, awesome_brattain_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_murdock_output, agitated_murdock_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(confident_mahavira_input_gpu);
    cudaFree(awesome_brattain_output_gpu);
    cudaFree(agitated_murdock_output_gpu);
}
        ","
void func_cpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){

    for (int elegant_ardinghelli = 0; elegant_ardinghelli <= N; elegant_ardinghelli++){
        awesome_brattain_output[elegant_ardinghelli] += confident_mahavira_input[elegant_ardinghelli];
    }
        
}
        ",0.000000,0.160000,0,10000,1000,1,1,10,1,1
39,"


__global__ void kernel(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int elegant_ardinghelli = bx*bdx + tx;
    if (elegant_ardinghelli < N){
        atomicAdd(&awesome_brattain_output[elegant_ardinghelli], confident_mahavira_input[elegant_ardinghelli]);
    }
    
        
}

void func_gpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * confident_mahavira_input_gpu;
    float * awesome_brattain_output_gpu;
    float * agitated_murdock_output_gpu;

    cudaMalloc(&confident_mahavira_input_gpu, size);
    cudaMalloc(&awesome_brattain_output_gpu, size);
    cudaMalloc(&agitated_murdock_output_gpu, size);

    cudaMemcpy(confident_mahavira_input_gpu, confident_mahavira_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(confident_mahavira_input_gpu, awesome_brattain_output_gpu, agitated_murdock_output_gpu, N);

    cudaMemcpy(awesome_brattain_output, awesome_brattain_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_murdock_output, agitated_murdock_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(confident_mahavira_input_gpu);
    cudaFree(awesome_brattain_output_gpu);
    cudaFree(agitated_murdock_output_gpu);
}
        ","
void func_cpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){

    for (int elegant_ardinghelli = 0; elegant_ardinghelli <= N; elegant_ardinghelli++){
        awesome_brattain_output[elegant_ardinghelli] += confident_mahavira_input[elegant_ardinghelli];
    }
        
}
        ",0.000000,0.160000,0,10000,100,1,1,100,1,1
40,"


__global__ void kernel(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int elegant_ardinghelli = bx*bdx + tx;
    if (elegant_ardinghelli < N){
        atomicAdd(&awesome_brattain_output[elegant_ardinghelli], confident_mahavira_input[elegant_ardinghelli]);
    }
    
        
}

void func_gpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * confident_mahavira_input_gpu;
    float * awesome_brattain_output_gpu;
    float * agitated_murdock_output_gpu;

    cudaMalloc(&confident_mahavira_input_gpu, size);
    cudaMalloc(&awesome_brattain_output_gpu, size);
    cudaMalloc(&agitated_murdock_output_gpu, size);

    cudaMemcpy(confident_mahavira_input_gpu, confident_mahavira_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(confident_mahavira_input_gpu, awesome_brattain_output_gpu, agitated_murdock_output_gpu, N);

    cudaMemcpy(awesome_brattain_output, awesome_brattain_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_murdock_output, agitated_murdock_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(confident_mahavira_input_gpu);
    cudaFree(awesome_brattain_output_gpu);
    cudaFree(agitated_murdock_output_gpu);
}
        ","
void func_cpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){

    for (int elegant_ardinghelli = 0; elegant_ardinghelli <= N; elegant_ardinghelli++){
        awesome_brattain_output[elegant_ardinghelli] += confident_mahavira_input[elegant_ardinghelli];
    }
        
}
        ",0.000000,0.160000,0,100000,10000,1,1,10,1,1
41,"


__global__ void kernel(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int elegant_ardinghelli = bx*bdx + tx;
    if (elegant_ardinghelli < N){
        atomicAdd(&awesome_brattain_output[elegant_ardinghelli], confident_mahavira_input[elegant_ardinghelli]);
    }
    
        
}

void func_gpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * confident_mahavira_input_gpu;
    float * awesome_brattain_output_gpu;
    float * agitated_murdock_output_gpu;

    cudaMalloc(&confident_mahavira_input_gpu, size);
    cudaMalloc(&awesome_brattain_output_gpu, size);
    cudaMalloc(&agitated_murdock_output_gpu, size);

    cudaMemcpy(confident_mahavira_input_gpu, confident_mahavira_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(confident_mahavira_input_gpu, awesome_brattain_output_gpu, agitated_murdock_output_gpu, N);

    cudaMemcpy(awesome_brattain_output, awesome_brattain_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_murdock_output, agitated_murdock_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(confident_mahavira_input_gpu);
    cudaFree(awesome_brattain_output_gpu);
    cudaFree(agitated_murdock_output_gpu);
}
        ","
void func_cpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){

    for (int elegant_ardinghelli = 0; elegant_ardinghelli <= N; elegant_ardinghelli++){
        awesome_brattain_output[elegant_ardinghelli] += confident_mahavira_input[elegant_ardinghelli];
    }
        
}
        ",0.000000,0.190000,0,100000,1000,1,1,100,1,1
42,"


__global__ void kernel(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int elegant_ardinghelli = bx*bdx + tx;
    if (elegant_ardinghelli < N){
        atomicAdd(&awesome_brattain_output[elegant_ardinghelli], confident_mahavira_input[elegant_ardinghelli]);
    }
    
        
}

void func_gpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * confident_mahavira_input_gpu;
    float * awesome_brattain_output_gpu;
    float * agitated_murdock_output_gpu;

    cudaMalloc(&confident_mahavira_input_gpu, size);
    cudaMalloc(&awesome_brattain_output_gpu, size);
    cudaMalloc(&agitated_murdock_output_gpu, size);

    cudaMemcpy(confident_mahavira_input_gpu, confident_mahavira_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(confident_mahavira_input_gpu, awesome_brattain_output_gpu, agitated_murdock_output_gpu, N);

    cudaMemcpy(awesome_brattain_output, awesome_brattain_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_murdock_output, agitated_murdock_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(confident_mahavira_input_gpu);
    cudaFree(awesome_brattain_output_gpu);
    cudaFree(agitated_murdock_output_gpu);
}
        ","
void func_cpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){

    for (int elegant_ardinghelli = 0; elegant_ardinghelli <= N; elegant_ardinghelli++){
        awesome_brattain_output[elegant_ardinghelli] += confident_mahavira_input[elegant_ardinghelli];
    }
        
}
        ",0.000000,0.160000,0,100000,100,1,1,1000,1,1
43,"


__global__ void kernel(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int elegant_ardinghelli = bx*bdx + tx;
    if (elegant_ardinghelli < N){
        atomicAdd(&awesome_brattain_output[elegant_ardinghelli], confident_mahavira_input[elegant_ardinghelli]);
    }
    
        
}

void func_gpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * confident_mahavira_input_gpu;
    float * awesome_brattain_output_gpu;
    float * agitated_murdock_output_gpu;

    cudaMalloc(&confident_mahavira_input_gpu, size);
    cudaMalloc(&awesome_brattain_output_gpu, size);
    cudaMalloc(&agitated_murdock_output_gpu, size);

    cudaMemcpy(confident_mahavira_input_gpu, confident_mahavira_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(confident_mahavira_input_gpu, awesome_brattain_output_gpu, agitated_murdock_output_gpu, N);

    cudaMemcpy(awesome_brattain_output, awesome_brattain_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_murdock_output, agitated_murdock_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(confident_mahavira_input_gpu);
    cudaFree(awesome_brattain_output_gpu);
    cudaFree(agitated_murdock_output_gpu);
}
        ","
void func_cpu(float * confident_mahavira_input, float * awesome_brattain_output, float * agitated_murdock_output, unsigned int N){

    for (int elegant_ardinghelli = 0; elegant_ardinghelli <= N; elegant_ardinghelli++){
        awesome_brattain_output[elegant_ardinghelli] += confident_mahavira_input[elegant_ardinghelli];
    }
        
}
        ",0.000000,0.170000,0,100000,1000,1,1,100,1,1
44,"


__global__ void kernel(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int infallible_hoover = bx*bdx + tx;
    if (infallible_hoover < N){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
    
        
}

void func_gpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * admiring_darwin_input_gpu;
    float * clever_edison_input_gpu;
    float * objective_mccarthy_output_gpu;
    float * clever_mclean_output_gpu;

    cudaMalloc(&admiring_darwin_input_gpu, size);
    cudaMalloc(&clever_edison_input_gpu, size);
    cudaMalloc(&objective_mccarthy_output_gpu, size);
    cudaMalloc(&clever_mclean_output_gpu, size);

    cudaMemcpy(admiring_darwin_input_gpu, admiring_darwin_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(clever_edison_input_gpu, clever_edison_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(admiring_darwin_input_gpu, clever_edison_input_gpu, objective_mccarthy_output_gpu, clever_mclean_output_gpu, N);

    cudaMemcpy(objective_mccarthy_output, objective_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(clever_mclean_output, clever_mclean_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(admiring_darwin_input_gpu);
    cudaFree(clever_edison_input_gpu);
    cudaFree(objective_mccarthy_output_gpu);
    cudaFree(clever_mclean_output_gpu);
}
        ","
void func_cpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){

    for (int infallible_hoover = 0; infallible_hoover <= N; infallible_hoover++){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
        
}
        ",0.000000,0.240000,0,10,10,1,1,1,1,1
45,"


__global__ void kernel(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int infallible_hoover = bx*bdx + tx;
    if (infallible_hoover < N){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
    
        
}

void func_gpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * admiring_darwin_input_gpu;
    float * clever_edison_input_gpu;
    float * objective_mccarthy_output_gpu;
    float * clever_mclean_output_gpu;

    cudaMalloc(&admiring_darwin_input_gpu, size);
    cudaMalloc(&clever_edison_input_gpu, size);
    cudaMalloc(&objective_mccarthy_output_gpu, size);
    cudaMalloc(&clever_mclean_output_gpu, size);

    cudaMemcpy(admiring_darwin_input_gpu, admiring_darwin_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(clever_edison_input_gpu, clever_edison_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(admiring_darwin_input_gpu, clever_edison_input_gpu, objective_mccarthy_output_gpu, clever_mclean_output_gpu, N);

    cudaMemcpy(objective_mccarthy_output, objective_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(clever_mclean_output, clever_mclean_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(admiring_darwin_input_gpu);
    cudaFree(clever_edison_input_gpu);
    cudaFree(objective_mccarthy_output_gpu);
    cudaFree(clever_mclean_output_gpu);
}
        ","
void func_cpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){

    for (int infallible_hoover = 0; infallible_hoover <= N; infallible_hoover++){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
        
}
        ",0.000000,0.170000,0,100,100,1,1,1,1,1
46,"


__global__ void kernel(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int infallible_hoover = bx*bdx + tx;
    if (infallible_hoover < N){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
    
        
}

void func_gpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * admiring_darwin_input_gpu;
    float * clever_edison_input_gpu;
    float * objective_mccarthy_output_gpu;
    float * clever_mclean_output_gpu;

    cudaMalloc(&admiring_darwin_input_gpu, size);
    cudaMalloc(&clever_edison_input_gpu, size);
    cudaMalloc(&objective_mccarthy_output_gpu, size);
    cudaMalloc(&clever_mclean_output_gpu, size);

    cudaMemcpy(admiring_darwin_input_gpu, admiring_darwin_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(clever_edison_input_gpu, clever_edison_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(admiring_darwin_input_gpu, clever_edison_input_gpu, objective_mccarthy_output_gpu, clever_mclean_output_gpu, N);

    cudaMemcpy(objective_mccarthy_output, objective_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(clever_mclean_output, clever_mclean_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(admiring_darwin_input_gpu);
    cudaFree(clever_edison_input_gpu);
    cudaFree(objective_mccarthy_output_gpu);
    cudaFree(clever_mclean_output_gpu);
}
        ","
void func_cpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){

    for (int infallible_hoover = 0; infallible_hoover <= N; infallible_hoover++){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
        
}
        ",0.000000,0.160000,0,100,10,1,1,10,1,1
47,"


__global__ void kernel(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int infallible_hoover = bx*bdx + tx;
    if (infallible_hoover < N){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
    
        
}

void func_gpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * admiring_darwin_input_gpu;
    float * clever_edison_input_gpu;
    float * objective_mccarthy_output_gpu;
    float * clever_mclean_output_gpu;

    cudaMalloc(&admiring_darwin_input_gpu, size);
    cudaMalloc(&clever_edison_input_gpu, size);
    cudaMalloc(&objective_mccarthy_output_gpu, size);
    cudaMalloc(&clever_mclean_output_gpu, size);

    cudaMemcpy(admiring_darwin_input_gpu, admiring_darwin_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(clever_edison_input_gpu, clever_edison_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(admiring_darwin_input_gpu, clever_edison_input_gpu, objective_mccarthy_output_gpu, clever_mclean_output_gpu, N);

    cudaMemcpy(objective_mccarthy_output, objective_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(clever_mclean_output, clever_mclean_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(admiring_darwin_input_gpu);
    cudaFree(clever_edison_input_gpu);
    cudaFree(objective_mccarthy_output_gpu);
    cudaFree(clever_mclean_output_gpu);
}
        ","
void func_cpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){

    for (int infallible_hoover = 0; infallible_hoover <= N; infallible_hoover++){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
        
}
        ",0.000000,0.190000,0,100,1,1,1,100,1,1
48,"


__global__ void kernel(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int infallible_hoover = bx*bdx + tx;
    if (infallible_hoover < N){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
    
        
}

void func_gpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * admiring_darwin_input_gpu;
    float * clever_edison_input_gpu;
    float * objective_mccarthy_output_gpu;
    float * clever_mclean_output_gpu;

    cudaMalloc(&admiring_darwin_input_gpu, size);
    cudaMalloc(&clever_edison_input_gpu, size);
    cudaMalloc(&objective_mccarthy_output_gpu, size);
    cudaMalloc(&clever_mclean_output_gpu, size);

    cudaMemcpy(admiring_darwin_input_gpu, admiring_darwin_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(clever_edison_input_gpu, clever_edison_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(admiring_darwin_input_gpu, clever_edison_input_gpu, objective_mccarthy_output_gpu, clever_mclean_output_gpu, N);

    cudaMemcpy(objective_mccarthy_output, objective_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(clever_mclean_output, clever_mclean_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(admiring_darwin_input_gpu);
    cudaFree(clever_edison_input_gpu);
    cudaFree(objective_mccarthy_output_gpu);
    cudaFree(clever_mclean_output_gpu);
}
        ","
void func_cpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){

    for (int infallible_hoover = 0; infallible_hoover <= N; infallible_hoover++){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
        
}
        ",0.000000,0.170000,0,10000,10000,1,1,1,1,1
49,"


__global__ void kernel(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int infallible_hoover = bx*bdx + tx;
    if (infallible_hoover < N){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
    
        
}

void func_gpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * admiring_darwin_input_gpu;
    float * clever_edison_input_gpu;
    float * objective_mccarthy_output_gpu;
    float * clever_mclean_output_gpu;

    cudaMalloc(&admiring_darwin_input_gpu, size);
    cudaMalloc(&clever_edison_input_gpu, size);
    cudaMalloc(&objective_mccarthy_output_gpu, size);
    cudaMalloc(&clever_mclean_output_gpu, size);

    cudaMemcpy(admiring_darwin_input_gpu, admiring_darwin_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(clever_edison_input_gpu, clever_edison_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(admiring_darwin_input_gpu, clever_edison_input_gpu, objective_mccarthy_output_gpu, clever_mclean_output_gpu, N);

    cudaMemcpy(objective_mccarthy_output, objective_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(clever_mclean_output, clever_mclean_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(admiring_darwin_input_gpu);
    cudaFree(clever_edison_input_gpu);
    cudaFree(objective_mccarthy_output_gpu);
    cudaFree(clever_mclean_output_gpu);
}
        ","
void func_cpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){

    for (int infallible_hoover = 0; infallible_hoover <= N; infallible_hoover++){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
        
}
        ",0.000000,0.170000,0,10000,1000,1,1,10,1,1
50,"


__global__ void kernel(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int infallible_hoover = bx*bdx + tx;
    if (infallible_hoover < N){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
    
        
}

void func_gpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * admiring_darwin_input_gpu;
    float * clever_edison_input_gpu;
    float * objective_mccarthy_output_gpu;
    float * clever_mclean_output_gpu;

    cudaMalloc(&admiring_darwin_input_gpu, size);
    cudaMalloc(&clever_edison_input_gpu, size);
    cudaMalloc(&objective_mccarthy_output_gpu, size);
    cudaMalloc(&clever_mclean_output_gpu, size);

    cudaMemcpy(admiring_darwin_input_gpu, admiring_darwin_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(clever_edison_input_gpu, clever_edison_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(admiring_darwin_input_gpu, clever_edison_input_gpu, objective_mccarthy_output_gpu, clever_mclean_output_gpu, N);

    cudaMemcpy(objective_mccarthy_output, objective_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(clever_mclean_output, clever_mclean_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(admiring_darwin_input_gpu);
    cudaFree(clever_edison_input_gpu);
    cudaFree(objective_mccarthy_output_gpu);
    cudaFree(clever_mclean_output_gpu);
}
        ","
void func_cpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){

    for (int infallible_hoover = 0; infallible_hoover <= N; infallible_hoover++){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
        
}
        ",0.000000,0.150000,0,10000,100,1,1,100,1,1
51,"


__global__ void kernel(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int infallible_hoover = bx*bdx + tx;
    if (infallible_hoover < N){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
    
        
}

void func_gpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * admiring_darwin_input_gpu;
    float * clever_edison_input_gpu;
    float * objective_mccarthy_output_gpu;
    float * clever_mclean_output_gpu;

    cudaMalloc(&admiring_darwin_input_gpu, size);
    cudaMalloc(&clever_edison_input_gpu, size);
    cudaMalloc(&objective_mccarthy_output_gpu, size);
    cudaMalloc(&clever_mclean_output_gpu, size);

    cudaMemcpy(admiring_darwin_input_gpu, admiring_darwin_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(clever_edison_input_gpu, clever_edison_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(admiring_darwin_input_gpu, clever_edison_input_gpu, objective_mccarthy_output_gpu, clever_mclean_output_gpu, N);

    cudaMemcpy(objective_mccarthy_output, objective_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(clever_mclean_output, clever_mclean_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(admiring_darwin_input_gpu);
    cudaFree(clever_edison_input_gpu);
    cudaFree(objective_mccarthy_output_gpu);
    cudaFree(clever_mclean_output_gpu);
}
        ","
void func_cpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){

    for (int infallible_hoover = 0; infallible_hoover <= N; infallible_hoover++){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
        
}
        ",0.000000,0.160000,0,100000,10000,1,1,10,1,1
52,"


__global__ void kernel(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int infallible_hoover = bx*bdx + tx;
    if (infallible_hoover < N){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
    
        
}

void func_gpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * admiring_darwin_input_gpu;
    float * clever_edison_input_gpu;
    float * objective_mccarthy_output_gpu;
    float * clever_mclean_output_gpu;

    cudaMalloc(&admiring_darwin_input_gpu, size);
    cudaMalloc(&clever_edison_input_gpu, size);
    cudaMalloc(&objective_mccarthy_output_gpu, size);
    cudaMalloc(&clever_mclean_output_gpu, size);

    cudaMemcpy(admiring_darwin_input_gpu, admiring_darwin_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(clever_edison_input_gpu, clever_edison_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(admiring_darwin_input_gpu, clever_edison_input_gpu, objective_mccarthy_output_gpu, clever_mclean_output_gpu, N);

    cudaMemcpy(objective_mccarthy_output, objective_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(clever_mclean_output, clever_mclean_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(admiring_darwin_input_gpu);
    cudaFree(clever_edison_input_gpu);
    cudaFree(objective_mccarthy_output_gpu);
    cudaFree(clever_mclean_output_gpu);
}
        ","
void func_cpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){

    for (int infallible_hoover = 0; infallible_hoover <= N; infallible_hoover++){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
        
}
        ",0.000000,0.170000,0,100000,1000,1,1,100,1,1
53,"


__global__ void kernel(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int infallible_hoover = bx*bdx + tx;
    if (infallible_hoover < N){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
    
        
}

void func_gpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * admiring_darwin_input_gpu;
    float * clever_edison_input_gpu;
    float * objective_mccarthy_output_gpu;
    float * clever_mclean_output_gpu;

    cudaMalloc(&admiring_darwin_input_gpu, size);
    cudaMalloc(&clever_edison_input_gpu, size);
    cudaMalloc(&objective_mccarthy_output_gpu, size);
    cudaMalloc(&clever_mclean_output_gpu, size);

    cudaMemcpy(admiring_darwin_input_gpu, admiring_darwin_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(clever_edison_input_gpu, clever_edison_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(admiring_darwin_input_gpu, clever_edison_input_gpu, objective_mccarthy_output_gpu, clever_mclean_output_gpu, N);

    cudaMemcpy(objective_mccarthy_output, objective_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(clever_mclean_output, clever_mclean_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(admiring_darwin_input_gpu);
    cudaFree(clever_edison_input_gpu);
    cudaFree(objective_mccarthy_output_gpu);
    cudaFree(clever_mclean_output_gpu);
}
        ","
void func_cpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){

    for (int infallible_hoover = 0; infallible_hoover <= N; infallible_hoover++){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
        
}
        ",0.000000,0.160000,0,100000,100,1,1,1000,1,1
54,"


__global__ void kernel(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int infallible_hoover = bx*bdx + tx;
    if (infallible_hoover < N){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
    
        
}

void func_gpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * admiring_darwin_input_gpu;
    float * clever_edison_input_gpu;
    float * objective_mccarthy_output_gpu;
    float * clever_mclean_output_gpu;

    cudaMalloc(&admiring_darwin_input_gpu, size);
    cudaMalloc(&clever_edison_input_gpu, size);
    cudaMalloc(&objective_mccarthy_output_gpu, size);
    cudaMalloc(&clever_mclean_output_gpu, size);

    cudaMemcpy(admiring_darwin_input_gpu, admiring_darwin_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(clever_edison_input_gpu, clever_edison_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(admiring_darwin_input_gpu, clever_edison_input_gpu, objective_mccarthy_output_gpu, clever_mclean_output_gpu, N);

    cudaMemcpy(objective_mccarthy_output, objective_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(clever_mclean_output, clever_mclean_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(admiring_darwin_input_gpu);
    cudaFree(clever_edison_input_gpu);
    cudaFree(objective_mccarthy_output_gpu);
    cudaFree(clever_mclean_output_gpu);
}
        ","
void func_cpu(float * admiring_darwin_input, float * clever_edison_input, float * objective_mccarthy_output, float * clever_mclean_output, unsigned int N){

    for (int infallible_hoover = 0; infallible_hoover <= N; infallible_hoover++){
        objective_mccarthy_output[infallible_hoover] += clever_edison_input[infallible_hoover];
    }
        
}
        ",0.000000,0.170000,0,100000,1000,1,1,100,1,1
55,"


__global__ void kernel(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int zealous_golick = bx*bdx + tx;
    if (zealous_golick < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * xenodochial_mahavira_input_gpu;
    float * confident_swartz_input_gpu;
    float * eloquent_shaw_output_gpu;
    float * kind_borg_output_gpu;

    cudaMalloc(&xenodochial_mahavira_input_gpu, size);
    cudaMalloc(&confident_swartz_input_gpu, size);
    cudaMalloc(&eloquent_shaw_output_gpu, size);
    cudaMalloc(&kind_borg_output_gpu, size);

    cudaMemcpy(xenodochial_mahavira_input_gpu, xenodochial_mahavira_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(confident_swartz_input_gpu, confident_swartz_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(xenodochial_mahavira_input_gpu, confident_swartz_input_gpu, eloquent_shaw_output_gpu, kind_borg_output_gpu, N);

    cudaMemcpy(eloquent_shaw_output, eloquent_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(kind_borg_output, kind_borg_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(xenodochial_mahavira_input_gpu);
    cudaFree(confident_swartz_input_gpu);
    cudaFree(eloquent_shaw_output_gpu);
    cudaFree(kind_borg_output_gpu);
}
        ","
void func_cpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){

    for (int zealous_golick = 0; zealous_golick <= N; zealous_golick++){

    }
        
}
        ",0.000000,0.240000,0,10,10,1,1,1,1,1
56,"


__global__ void kernel(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int zealous_golick = bx*bdx + tx;
    if (zealous_golick < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * xenodochial_mahavira_input_gpu;
    float * confident_swartz_input_gpu;
    float * eloquent_shaw_output_gpu;
    float * kind_borg_output_gpu;

    cudaMalloc(&xenodochial_mahavira_input_gpu, size);
    cudaMalloc(&confident_swartz_input_gpu, size);
    cudaMalloc(&eloquent_shaw_output_gpu, size);
    cudaMalloc(&kind_borg_output_gpu, size);

    cudaMemcpy(xenodochial_mahavira_input_gpu, xenodochial_mahavira_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(confident_swartz_input_gpu, confident_swartz_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(xenodochial_mahavira_input_gpu, confident_swartz_input_gpu, eloquent_shaw_output_gpu, kind_borg_output_gpu, N);

    cudaMemcpy(eloquent_shaw_output, eloquent_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(kind_borg_output, kind_borg_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(xenodochial_mahavira_input_gpu);
    cudaFree(confident_swartz_input_gpu);
    cudaFree(eloquent_shaw_output_gpu);
    cudaFree(kind_borg_output_gpu);
}
        ","
void func_cpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){

    for (int zealous_golick = 0; zealous_golick <= N; zealous_golick++){

    }
        
}
        ",0.000000,0.180000,0,100,100,1,1,1,1,1
57,"


__global__ void kernel(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int zealous_golick = bx*bdx + tx;
    if (zealous_golick < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * xenodochial_mahavira_input_gpu;
    float * confident_swartz_input_gpu;
    float * eloquent_shaw_output_gpu;
    float * kind_borg_output_gpu;

    cudaMalloc(&xenodochial_mahavira_input_gpu, size);
    cudaMalloc(&confident_swartz_input_gpu, size);
    cudaMalloc(&eloquent_shaw_output_gpu, size);
    cudaMalloc(&kind_borg_output_gpu, size);

    cudaMemcpy(xenodochial_mahavira_input_gpu, xenodochial_mahavira_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(confident_swartz_input_gpu, confident_swartz_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(xenodochial_mahavira_input_gpu, confident_swartz_input_gpu, eloquent_shaw_output_gpu, kind_borg_output_gpu, N);

    cudaMemcpy(eloquent_shaw_output, eloquent_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(kind_borg_output, kind_borg_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(xenodochial_mahavira_input_gpu);
    cudaFree(confident_swartz_input_gpu);
    cudaFree(eloquent_shaw_output_gpu);
    cudaFree(kind_borg_output_gpu);
}
        ","
void func_cpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){

    for (int zealous_golick = 0; zealous_golick <= N; zealous_golick++){

    }
        
}
        ",0.000000,0.160000,0,100,10,1,1,10,1,1
58,"


__global__ void kernel(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int zealous_golick = bx*bdx + tx;
    if (zealous_golick < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * xenodochial_mahavira_input_gpu;
    float * confident_swartz_input_gpu;
    float * eloquent_shaw_output_gpu;
    float * kind_borg_output_gpu;

    cudaMalloc(&xenodochial_mahavira_input_gpu, size);
    cudaMalloc(&confident_swartz_input_gpu, size);
    cudaMalloc(&eloquent_shaw_output_gpu, size);
    cudaMalloc(&kind_borg_output_gpu, size);

    cudaMemcpy(xenodochial_mahavira_input_gpu, xenodochial_mahavira_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(confident_swartz_input_gpu, confident_swartz_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(xenodochial_mahavira_input_gpu, confident_swartz_input_gpu, eloquent_shaw_output_gpu, kind_borg_output_gpu, N);

    cudaMemcpy(eloquent_shaw_output, eloquent_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(kind_borg_output, kind_borg_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(xenodochial_mahavira_input_gpu);
    cudaFree(confident_swartz_input_gpu);
    cudaFree(eloquent_shaw_output_gpu);
    cudaFree(kind_borg_output_gpu);
}
        ","
void func_cpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){

    for (int zealous_golick = 0; zealous_golick <= N; zealous_golick++){

    }
        
}
        ",0.000000,0.160000,0,100,1,1,1,100,1,1
59,"


__global__ void kernel(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int zealous_golick = bx*bdx + tx;
    if (zealous_golick < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * xenodochial_mahavira_input_gpu;
    float * confident_swartz_input_gpu;
    float * eloquent_shaw_output_gpu;
    float * kind_borg_output_gpu;

    cudaMalloc(&xenodochial_mahavira_input_gpu, size);
    cudaMalloc(&confident_swartz_input_gpu, size);
    cudaMalloc(&eloquent_shaw_output_gpu, size);
    cudaMalloc(&kind_borg_output_gpu, size);

    cudaMemcpy(xenodochial_mahavira_input_gpu, xenodochial_mahavira_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(confident_swartz_input_gpu, confident_swartz_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(xenodochial_mahavira_input_gpu, confident_swartz_input_gpu, eloquent_shaw_output_gpu, kind_borg_output_gpu, N);

    cudaMemcpy(eloquent_shaw_output, eloquent_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(kind_borg_output, kind_borg_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(xenodochial_mahavira_input_gpu);
    cudaFree(confident_swartz_input_gpu);
    cudaFree(eloquent_shaw_output_gpu);
    cudaFree(kind_borg_output_gpu);
}
        ","
void func_cpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){

    for (int zealous_golick = 0; zealous_golick <= N; zealous_golick++){

    }
        
}
        ",0.000000,0.170000,0,10000,10000,1,1,1,1,1
60,"


__global__ void kernel(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int zealous_golick = bx*bdx + tx;
    if (zealous_golick < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * xenodochial_mahavira_input_gpu;
    float * confident_swartz_input_gpu;
    float * eloquent_shaw_output_gpu;
    float * kind_borg_output_gpu;

    cudaMalloc(&xenodochial_mahavira_input_gpu, size);
    cudaMalloc(&confident_swartz_input_gpu, size);
    cudaMalloc(&eloquent_shaw_output_gpu, size);
    cudaMalloc(&kind_borg_output_gpu, size);

    cudaMemcpy(xenodochial_mahavira_input_gpu, xenodochial_mahavira_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(confident_swartz_input_gpu, confident_swartz_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(xenodochial_mahavira_input_gpu, confident_swartz_input_gpu, eloquent_shaw_output_gpu, kind_borg_output_gpu, N);

    cudaMemcpy(eloquent_shaw_output, eloquent_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(kind_borg_output, kind_borg_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(xenodochial_mahavira_input_gpu);
    cudaFree(confident_swartz_input_gpu);
    cudaFree(eloquent_shaw_output_gpu);
    cudaFree(kind_borg_output_gpu);
}
        ","
void func_cpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){

    for (int zealous_golick = 0; zealous_golick <= N; zealous_golick++){

    }
        
}
        ",0.000000,0.170000,0,10000,1000,1,1,10,1,1
61,"


__global__ void kernel(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int zealous_golick = bx*bdx + tx;
    if (zealous_golick < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * xenodochial_mahavira_input_gpu;
    float * confident_swartz_input_gpu;
    float * eloquent_shaw_output_gpu;
    float * kind_borg_output_gpu;

    cudaMalloc(&xenodochial_mahavira_input_gpu, size);
    cudaMalloc(&confident_swartz_input_gpu, size);
    cudaMalloc(&eloquent_shaw_output_gpu, size);
    cudaMalloc(&kind_borg_output_gpu, size);

    cudaMemcpy(xenodochial_mahavira_input_gpu, xenodochial_mahavira_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(confident_swartz_input_gpu, confident_swartz_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(xenodochial_mahavira_input_gpu, confident_swartz_input_gpu, eloquent_shaw_output_gpu, kind_borg_output_gpu, N);

    cudaMemcpy(eloquent_shaw_output, eloquent_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(kind_borg_output, kind_borg_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(xenodochial_mahavira_input_gpu);
    cudaFree(confident_swartz_input_gpu);
    cudaFree(eloquent_shaw_output_gpu);
    cudaFree(kind_borg_output_gpu);
}
        ","
void func_cpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){

    for (int zealous_golick = 0; zealous_golick <= N; zealous_golick++){

    }
        
}
        ",0.000000,0.160000,0,10000,100,1,1,100,1,1
62,"


__global__ void kernel(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int zealous_golick = bx*bdx + tx;
    if (zealous_golick < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * xenodochial_mahavira_input_gpu;
    float * confident_swartz_input_gpu;
    float * eloquent_shaw_output_gpu;
    float * kind_borg_output_gpu;

    cudaMalloc(&xenodochial_mahavira_input_gpu, size);
    cudaMalloc(&confident_swartz_input_gpu, size);
    cudaMalloc(&eloquent_shaw_output_gpu, size);
    cudaMalloc(&kind_borg_output_gpu, size);

    cudaMemcpy(xenodochial_mahavira_input_gpu, xenodochial_mahavira_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(confident_swartz_input_gpu, confident_swartz_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(xenodochial_mahavira_input_gpu, confident_swartz_input_gpu, eloquent_shaw_output_gpu, kind_borg_output_gpu, N);

    cudaMemcpy(eloquent_shaw_output, eloquent_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(kind_borg_output, kind_borg_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(xenodochial_mahavira_input_gpu);
    cudaFree(confident_swartz_input_gpu);
    cudaFree(eloquent_shaw_output_gpu);
    cudaFree(kind_borg_output_gpu);
}
        ","
void func_cpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){

    for (int zealous_golick = 0; zealous_golick <= N; zealous_golick++){

    }
        
}
        ",0.000000,0.170000,0,100000,10000,1,1,10,1,1
63,"


__global__ void kernel(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int zealous_golick = bx*bdx + tx;
    if (zealous_golick < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * xenodochial_mahavira_input_gpu;
    float * confident_swartz_input_gpu;
    float * eloquent_shaw_output_gpu;
    float * kind_borg_output_gpu;

    cudaMalloc(&xenodochial_mahavira_input_gpu, size);
    cudaMalloc(&confident_swartz_input_gpu, size);
    cudaMalloc(&eloquent_shaw_output_gpu, size);
    cudaMalloc(&kind_borg_output_gpu, size);

    cudaMemcpy(xenodochial_mahavira_input_gpu, xenodochial_mahavira_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(confident_swartz_input_gpu, confident_swartz_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(xenodochial_mahavira_input_gpu, confident_swartz_input_gpu, eloquent_shaw_output_gpu, kind_borg_output_gpu, N);

    cudaMemcpy(eloquent_shaw_output, eloquent_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(kind_borg_output, kind_borg_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(xenodochial_mahavira_input_gpu);
    cudaFree(confident_swartz_input_gpu);
    cudaFree(eloquent_shaw_output_gpu);
    cudaFree(kind_borg_output_gpu);
}
        ","
void func_cpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){

    for (int zealous_golick = 0; zealous_golick <= N; zealous_golick++){

    }
        
}
        ",0.000000,0.160000,0,100000,1000,1,1,100,1,1
64,"


__global__ void kernel(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int zealous_golick = bx*bdx + tx;
    if (zealous_golick < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * xenodochial_mahavira_input_gpu;
    float * confident_swartz_input_gpu;
    float * eloquent_shaw_output_gpu;
    float * kind_borg_output_gpu;

    cudaMalloc(&xenodochial_mahavira_input_gpu, size);
    cudaMalloc(&confident_swartz_input_gpu, size);
    cudaMalloc(&eloquent_shaw_output_gpu, size);
    cudaMalloc(&kind_borg_output_gpu, size);

    cudaMemcpy(xenodochial_mahavira_input_gpu, xenodochial_mahavira_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(confident_swartz_input_gpu, confident_swartz_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(xenodochial_mahavira_input_gpu, confident_swartz_input_gpu, eloquent_shaw_output_gpu, kind_borg_output_gpu, N);

    cudaMemcpy(eloquent_shaw_output, eloquent_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(kind_borg_output, kind_borg_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(xenodochial_mahavira_input_gpu);
    cudaFree(confident_swartz_input_gpu);
    cudaFree(eloquent_shaw_output_gpu);
    cudaFree(kind_borg_output_gpu);
}
        ","
void func_cpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){

    for (int zealous_golick = 0; zealous_golick <= N; zealous_golick++){

    }
        
}
        ",0.000000,0.180000,0,100000,100,1,1,1000,1,1
65,"


__global__ void kernel(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int zealous_golick = bx*bdx + tx;
    if (zealous_golick < N){
        __syncthreads();
    }
    
        
}

void func_gpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * xenodochial_mahavira_input_gpu;
    float * confident_swartz_input_gpu;
    float * eloquent_shaw_output_gpu;
    float * kind_borg_output_gpu;

    cudaMalloc(&xenodochial_mahavira_input_gpu, size);
    cudaMalloc(&confident_swartz_input_gpu, size);
    cudaMalloc(&eloquent_shaw_output_gpu, size);
    cudaMalloc(&kind_borg_output_gpu, size);

    cudaMemcpy(xenodochial_mahavira_input_gpu, xenodochial_mahavira_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(confident_swartz_input_gpu, confident_swartz_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(xenodochial_mahavira_input_gpu, confident_swartz_input_gpu, eloquent_shaw_output_gpu, kind_borg_output_gpu, N);

    cudaMemcpy(eloquent_shaw_output, eloquent_shaw_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(kind_borg_output, kind_borg_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(xenodochial_mahavira_input_gpu);
    cudaFree(confident_swartz_input_gpu);
    cudaFree(eloquent_shaw_output_gpu);
    cudaFree(kind_borg_output_gpu);
}
        ","
void func_cpu(float * xenodochial_mahavira_input, float * confident_swartz_input, float * eloquent_shaw_output, float * kind_borg_output, unsigned int N){

    for (int zealous_golick = 0; zealous_golick <= N; zealous_golick++){

    }
        
}
        ",0.000000,0.190000,0,100000,1000,1,1,100,1,1
66,"


__global__ void kernel(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int serene_engelbart = bx*bdx + tx;
    if (serene_engelbart < N){

        int blissful_morse = by*bdy + ty;
        if (blissful_morse < N){
 
            __shared__ float zealous_ritchie[1024];

        }
        
        
    }
    
        
}

void func_gpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * thirsty_beaver_input_gpu;
    float * stupefied_neumann_output_gpu;
    float * angry_einstein_output_gpu;

    cudaMalloc(&thirsty_beaver_input_gpu, size);
    cudaMalloc(&stupefied_neumann_output_gpu, size);
    cudaMalloc(&angry_einstein_output_gpu, size);

    cudaMemcpy(thirsty_beaver_input_gpu, thirsty_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(thirsty_beaver_input_gpu, stupefied_neumann_output_gpu, angry_einstein_output_gpu, N);

    cudaMemcpy(stupefied_neumann_output, stupefied_neumann_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(angry_einstein_output, angry_einstein_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(thirsty_beaver_input_gpu);
    cudaFree(stupefied_neumann_output_gpu);
    cudaFree(angry_einstein_output_gpu);
}
        ","
void func_cpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){

    for (int serene_engelbart = 0; serene_engelbart <= N; serene_engelbart++){

        for (int blissful_morse = 0; blissful_morse <= N; blissful_morse++){

            float *zealous_ritchie;
            zealous_ritchie = (float *)malloc(N*sizeof(float));

        }
        
    }
        
}
        ",0.000000,0.170000,0,10,10,10,1,1,1,1
67,"


__global__ void kernel(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int serene_engelbart = bx*bdx + tx;
    if (serene_engelbart < N){

        int blissful_morse = by*bdy + ty;
        if (blissful_morse < N){
 
            __shared__ float zealous_ritchie[1024];

        }
        
        
    }
    
        
}

void func_gpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * thirsty_beaver_input_gpu;
    float * stupefied_neumann_output_gpu;
    float * angry_einstein_output_gpu;

    cudaMalloc(&thirsty_beaver_input_gpu, size);
    cudaMalloc(&stupefied_neumann_output_gpu, size);
    cudaMalloc(&angry_einstein_output_gpu, size);

    cudaMemcpy(thirsty_beaver_input_gpu, thirsty_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(thirsty_beaver_input_gpu, stupefied_neumann_output_gpu, angry_einstein_output_gpu, N);

    cudaMemcpy(stupefied_neumann_output, stupefied_neumann_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(angry_einstein_output, angry_einstein_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(thirsty_beaver_input_gpu);
    cudaFree(stupefied_neumann_output_gpu);
    cudaFree(angry_einstein_output_gpu);
}
        ","
void func_cpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){

    for (int serene_engelbart = 0; serene_engelbart <= N; serene_engelbart++){

        for (int blissful_morse = 0; blissful_morse <= N; blissful_morse++){

            float *zealous_ritchie;
            zealous_ritchie = (float *)malloc(N*sizeof(float));

        }
        
    }
        
}
        ",0.000000,0.180000,0,100,100,100,1,1,1,1
68,"


__global__ void kernel(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int serene_engelbart = bx*bdx + tx;
    if (serene_engelbart < N){

        int blissful_morse = by*bdy + ty;
        if (blissful_morse < N){
 
            __shared__ float zealous_ritchie[1024];

        }
        
        
    }
    
        
}

void func_gpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * thirsty_beaver_input_gpu;
    float * stupefied_neumann_output_gpu;
    float * angry_einstein_output_gpu;

    cudaMalloc(&thirsty_beaver_input_gpu, size);
    cudaMalloc(&stupefied_neumann_output_gpu, size);
    cudaMalloc(&angry_einstein_output_gpu, size);

    cudaMemcpy(thirsty_beaver_input_gpu, thirsty_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(thirsty_beaver_input_gpu, stupefied_neumann_output_gpu, angry_einstein_output_gpu, N);

    cudaMemcpy(stupefied_neumann_output, stupefied_neumann_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(angry_einstein_output, angry_einstein_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(thirsty_beaver_input_gpu);
    cudaFree(stupefied_neumann_output_gpu);
    cudaFree(angry_einstein_output_gpu);
}
        ","
void func_cpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){

    for (int serene_engelbart = 0; serene_engelbart <= N; serene_engelbart++){

        for (int blissful_morse = 0; blissful_morse <= N; blissful_morse++){

            float *zealous_ritchie;
            zealous_ritchie = (float *)malloc(N*sizeof(float));

        }
        
    }
        
}
        ",0.000000,0.160000,0,100,10,10,1,10,10,1
69,"


__global__ void kernel(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int serene_engelbart = bx*bdx + tx;
    if (serene_engelbart < N){

        int blissful_morse = by*bdy + ty;
        if (blissful_morse < N){
 
            __shared__ float zealous_ritchie[1024];

        }
        
        
    }
    
        
}

void func_gpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * thirsty_beaver_input_gpu;
    float * stupefied_neumann_output_gpu;
    float * angry_einstein_output_gpu;

    cudaMalloc(&thirsty_beaver_input_gpu, size);
    cudaMalloc(&stupefied_neumann_output_gpu, size);
    cudaMalloc(&angry_einstein_output_gpu, size);

    cudaMemcpy(thirsty_beaver_input_gpu, thirsty_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(thirsty_beaver_input_gpu, stupefied_neumann_output_gpu, angry_einstein_output_gpu, N);

    cudaMemcpy(stupefied_neumann_output, stupefied_neumann_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(angry_einstein_output, angry_einstein_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(thirsty_beaver_input_gpu);
    cudaFree(stupefied_neumann_output_gpu);
    cudaFree(angry_einstein_output_gpu);
}
        ","
void func_cpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){

    for (int serene_engelbart = 0; serene_engelbart <= N; serene_engelbart++){

        for (int blissful_morse = 0; blissful_morse <= N; blissful_morse++){

            float *zealous_ritchie;
            zealous_ritchie = (float *)malloc(N*sizeof(float));

        }
        
    }
        
}
        ",0.000000,0.160000,0,100,1,1,1,100,100,1
70,"


__global__ void kernel(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int serene_engelbart = bx*bdx + tx;
    if (serene_engelbart < N){

        int blissful_morse = by*bdy + ty;
        if (blissful_morse < N){
 
            __shared__ float zealous_ritchie[1024];

        }
        
        
    }
    
        
}

void func_gpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * thirsty_beaver_input_gpu;
    float * stupefied_neumann_output_gpu;
    float * angry_einstein_output_gpu;

    cudaMalloc(&thirsty_beaver_input_gpu, size);
    cudaMalloc(&stupefied_neumann_output_gpu, size);
    cudaMalloc(&angry_einstein_output_gpu, size);

    cudaMemcpy(thirsty_beaver_input_gpu, thirsty_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(thirsty_beaver_input_gpu, stupefied_neumann_output_gpu, angry_einstein_output_gpu, N);

    cudaMemcpy(stupefied_neumann_output, stupefied_neumann_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(angry_einstein_output, angry_einstein_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(thirsty_beaver_input_gpu);
    cudaFree(stupefied_neumann_output_gpu);
    cudaFree(angry_einstein_output_gpu);
}
        ","
void func_cpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){

    for (int serene_engelbart = 0; serene_engelbart <= N; serene_engelbart++){

        for (int blissful_morse = 0; blissful_morse <= N; blissful_morse++){

            float *zealous_ritchie;
            zealous_ritchie = (float *)malloc(N*sizeof(float));

        }
        
    }
        
}
        ",2.660000,0.170000,0,1000,1000,1000,1,1,1,1
71,"


__global__ void kernel(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int serene_engelbart = bx*bdx + tx;
    if (serene_engelbart < N){

        int blissful_morse = by*bdy + ty;
        if (blissful_morse < N){
 
            __shared__ float zealous_ritchie[1024];

        }
        
        
    }
    
        
}

void func_gpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * thirsty_beaver_input_gpu;
    float * stupefied_neumann_output_gpu;
    float * angry_einstein_output_gpu;

    cudaMalloc(&thirsty_beaver_input_gpu, size);
    cudaMalloc(&stupefied_neumann_output_gpu, size);
    cudaMalloc(&angry_einstein_output_gpu, size);

    cudaMemcpy(thirsty_beaver_input_gpu, thirsty_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(thirsty_beaver_input_gpu, stupefied_neumann_output_gpu, angry_einstein_output_gpu, N);

    cudaMemcpy(stupefied_neumann_output, stupefied_neumann_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(angry_einstein_output, angry_einstein_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(thirsty_beaver_input_gpu);
    cudaFree(stupefied_neumann_output_gpu);
    cudaFree(angry_einstein_output_gpu);
}
        ","
void func_cpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){

    for (int serene_engelbart = 0; serene_engelbart <= N; serene_engelbart++){

        for (int blissful_morse = 0; blissful_morse <= N; blissful_morse++){

            float *zealous_ritchie;
            zealous_ritchie = (float *)malloc(N*sizeof(float));

        }
        
    }
        
}
        ",2.640000,0.180000,0,1000,100,100,1,10,10,1
72,"


__global__ void kernel(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int serene_engelbart = bx*bdx + tx;
    if (serene_engelbart < N){

        int blissful_morse = by*bdy + ty;
        if (blissful_morse < N){
 
            __shared__ float zealous_ritchie[1024];

        }
        
        
    }
    
        
}

void func_gpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * thirsty_beaver_input_gpu;
    float * stupefied_neumann_output_gpu;
    float * angry_einstein_output_gpu;

    cudaMalloc(&thirsty_beaver_input_gpu, size);
    cudaMalloc(&stupefied_neumann_output_gpu, size);
    cudaMalloc(&angry_einstein_output_gpu, size);

    cudaMemcpy(thirsty_beaver_input_gpu, thirsty_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(thirsty_beaver_input_gpu, stupefied_neumann_output_gpu, angry_einstein_output_gpu, N);

    cudaMemcpy(stupefied_neumann_output, stupefied_neumann_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(angry_einstein_output, angry_einstein_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(thirsty_beaver_input_gpu);
    cudaFree(stupefied_neumann_output_gpu);
    cudaFree(angry_einstein_output_gpu);
}
        ","
void func_cpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){

    for (int serene_engelbart = 0; serene_engelbart <= N; serene_engelbart++){

        for (int blissful_morse = 0; blissful_morse <= N; blissful_morse++){

            float *zealous_ritchie;
            zealous_ritchie = (float *)malloc(N*sizeof(float));

        }
        
    }
        
}
        ",71.310000,0.270000,0,5000,5000,5000,1,1,1,1
73,"


__global__ void kernel(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int serene_engelbart = bx*bdx + tx;
    if (serene_engelbart < N){

        int blissful_morse = by*bdy + ty;
        if (blissful_morse < N){
 
            __shared__ float zealous_ritchie[1024];

        }
        
        
    }
    
        
}

void func_gpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * thirsty_beaver_input_gpu;
    float * stupefied_neumann_output_gpu;
    float * angry_einstein_output_gpu;

    cudaMalloc(&thirsty_beaver_input_gpu, size);
    cudaMalloc(&stupefied_neumann_output_gpu, size);
    cudaMalloc(&angry_einstein_output_gpu, size);

    cudaMemcpy(thirsty_beaver_input_gpu, thirsty_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(thirsty_beaver_input_gpu, stupefied_neumann_output_gpu, angry_einstein_output_gpu, N);

    cudaMemcpy(stupefied_neumann_output, stupefied_neumann_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(angry_einstein_output, angry_einstein_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(thirsty_beaver_input_gpu);
    cudaFree(stupefied_neumann_output_gpu);
    cudaFree(angry_einstein_output_gpu);
}
        ","
void func_cpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){

    for (int serene_engelbart = 0; serene_engelbart <= N; serene_engelbart++){

        for (int blissful_morse = 0; blissful_morse <= N; blissful_morse++){

            float *zealous_ritchie;
            zealous_ritchie = (float *)malloc(N*sizeof(float));

        }
        
    }
        
}
        ",73.090000,0.320000,0,5000,500,500,1,10,10,1
74,"


__global__ void kernel(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int serene_engelbart = bx*bdx + tx;
    if (serene_engelbart < N){

        int blissful_morse = by*bdy + ty;
        if (blissful_morse < N){
 
            __shared__ float zealous_ritchie[1024];

        }
        
        
    }
    
        
}

void func_gpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * thirsty_beaver_input_gpu;
    float * stupefied_neumann_output_gpu;
    float * angry_einstein_output_gpu;

    cudaMalloc(&thirsty_beaver_input_gpu, size);
    cudaMalloc(&stupefied_neumann_output_gpu, size);
    cudaMalloc(&angry_einstein_output_gpu, size);

    cudaMemcpy(thirsty_beaver_input_gpu, thirsty_beaver_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(thirsty_beaver_input_gpu, stupefied_neumann_output_gpu, angry_einstein_output_gpu, N);

    cudaMemcpy(stupefied_neumann_output, stupefied_neumann_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(angry_einstein_output, angry_einstein_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(thirsty_beaver_input_gpu);
    cudaFree(stupefied_neumann_output_gpu);
    cudaFree(angry_einstein_output_gpu);
}
        ","
void func_cpu(float * thirsty_beaver_input, float * stupefied_neumann_output, float * angry_einstein_output, unsigned int N){

    for (int serene_engelbart = 0; serene_engelbart <= N; serene_engelbart++){

        for (int blissful_morse = 0; blissful_morse <= N; blissful_morse++){

            float *zealous_ritchie;
            zealous_ritchie = (float *)malloc(N*sizeof(float));

        }
        
    }
        
}
        ",71.100000,0.210000,0,5000,50,50,1,100,100,1
75,"


__global__ void kernel(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int eloquent_kepler = bx*bdx + tx;
    if (eloquent_kepler < N){
 
        __shared__ float nifty_gates[1024];

    }
    
        
}

void func_gpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vigorous_wing_input_gpu;
    float * unruffled_darwin_input_gpu;
    float * sleepy_mccarthy_output_gpu;
    float * agitated_johnson_output_gpu;

    cudaMalloc(&vigorous_wing_input_gpu, size);
    cudaMalloc(&unruffled_darwin_input_gpu, size);
    cudaMalloc(&sleepy_mccarthy_output_gpu, size);
    cudaMalloc(&agitated_johnson_output_gpu, size);

    cudaMemcpy(vigorous_wing_input_gpu, vigorous_wing_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(unruffled_darwin_input_gpu, unruffled_darwin_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vigorous_wing_input_gpu, unruffled_darwin_input_gpu, sleepy_mccarthy_output_gpu, agitated_johnson_output_gpu, N);

    cudaMemcpy(sleepy_mccarthy_output, sleepy_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_johnson_output, agitated_johnson_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vigorous_wing_input_gpu);
    cudaFree(unruffled_darwin_input_gpu);
    cudaFree(sleepy_mccarthy_output_gpu);
    cudaFree(agitated_johnson_output_gpu);
}
        ","
void func_cpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){

    for (int eloquent_kepler = 0; eloquent_kepler <= N; eloquent_kepler++){

        float *nifty_gates;
        nifty_gates = (float *)malloc(N*sizeof(float));

    }
        
}
        ",0.000000,0.250000,0,10,10,1,1,1,1,1
76,"


__global__ void kernel(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int eloquent_kepler = bx*bdx + tx;
    if (eloquent_kepler < N){
 
        __shared__ float nifty_gates[1024];

    }
    
        
}

void func_gpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vigorous_wing_input_gpu;
    float * unruffled_darwin_input_gpu;
    float * sleepy_mccarthy_output_gpu;
    float * agitated_johnson_output_gpu;

    cudaMalloc(&vigorous_wing_input_gpu, size);
    cudaMalloc(&unruffled_darwin_input_gpu, size);
    cudaMalloc(&sleepy_mccarthy_output_gpu, size);
    cudaMalloc(&agitated_johnson_output_gpu, size);

    cudaMemcpy(vigorous_wing_input_gpu, vigorous_wing_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(unruffled_darwin_input_gpu, unruffled_darwin_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vigorous_wing_input_gpu, unruffled_darwin_input_gpu, sleepy_mccarthy_output_gpu, agitated_johnson_output_gpu, N);

    cudaMemcpy(sleepy_mccarthy_output, sleepy_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_johnson_output, agitated_johnson_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vigorous_wing_input_gpu);
    cudaFree(unruffled_darwin_input_gpu);
    cudaFree(sleepy_mccarthy_output_gpu);
    cudaFree(agitated_johnson_output_gpu);
}
        ","
void func_cpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){

    for (int eloquent_kepler = 0; eloquent_kepler <= N; eloquent_kepler++){

        float *nifty_gates;
        nifty_gates = (float *)malloc(N*sizeof(float));

    }
        
}
        ",0.000000,0.170000,0,100,100,1,1,1,1,1
77,"


__global__ void kernel(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int eloquent_kepler = bx*bdx + tx;
    if (eloquent_kepler < N){
 
        __shared__ float nifty_gates[1024];

    }
    
        
}

void func_gpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vigorous_wing_input_gpu;
    float * unruffled_darwin_input_gpu;
    float * sleepy_mccarthy_output_gpu;
    float * agitated_johnson_output_gpu;

    cudaMalloc(&vigorous_wing_input_gpu, size);
    cudaMalloc(&unruffled_darwin_input_gpu, size);
    cudaMalloc(&sleepy_mccarthy_output_gpu, size);
    cudaMalloc(&agitated_johnson_output_gpu, size);

    cudaMemcpy(vigorous_wing_input_gpu, vigorous_wing_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(unruffled_darwin_input_gpu, unruffled_darwin_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vigorous_wing_input_gpu, unruffled_darwin_input_gpu, sleepy_mccarthy_output_gpu, agitated_johnson_output_gpu, N);

    cudaMemcpy(sleepy_mccarthy_output, sleepy_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_johnson_output, agitated_johnson_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vigorous_wing_input_gpu);
    cudaFree(unruffled_darwin_input_gpu);
    cudaFree(sleepy_mccarthy_output_gpu);
    cudaFree(agitated_johnson_output_gpu);
}
        ","
void func_cpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){

    for (int eloquent_kepler = 0; eloquent_kepler <= N; eloquent_kepler++){

        float *nifty_gates;
        nifty_gates = (float *)malloc(N*sizeof(float));

    }
        
}
        ",0.000000,0.160000,0,100,10,1,1,10,1,1
78,"


__global__ void kernel(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int eloquent_kepler = bx*bdx + tx;
    if (eloquent_kepler < N){
 
        __shared__ float nifty_gates[1024];

    }
    
        
}

void func_gpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vigorous_wing_input_gpu;
    float * unruffled_darwin_input_gpu;
    float * sleepy_mccarthy_output_gpu;
    float * agitated_johnson_output_gpu;

    cudaMalloc(&vigorous_wing_input_gpu, size);
    cudaMalloc(&unruffled_darwin_input_gpu, size);
    cudaMalloc(&sleepy_mccarthy_output_gpu, size);
    cudaMalloc(&agitated_johnson_output_gpu, size);

    cudaMemcpy(vigorous_wing_input_gpu, vigorous_wing_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(unruffled_darwin_input_gpu, unruffled_darwin_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vigorous_wing_input_gpu, unruffled_darwin_input_gpu, sleepy_mccarthy_output_gpu, agitated_johnson_output_gpu, N);

    cudaMemcpy(sleepy_mccarthy_output, sleepy_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_johnson_output, agitated_johnson_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vigorous_wing_input_gpu);
    cudaFree(unruffled_darwin_input_gpu);
    cudaFree(sleepy_mccarthy_output_gpu);
    cudaFree(agitated_johnson_output_gpu);
}
        ","
void func_cpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){

    for (int eloquent_kepler = 0; eloquent_kepler <= N; eloquent_kepler++){

        float *nifty_gates;
        nifty_gates = (float *)malloc(N*sizeof(float));

    }
        
}
        ",0.000000,0.150000,0,100,1,1,1,100,1,1
79,"


__global__ void kernel(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int eloquent_kepler = bx*bdx + tx;
    if (eloquent_kepler < N){
 
        __shared__ float nifty_gates[1024];

    }
    
        
}

void func_gpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vigorous_wing_input_gpu;
    float * unruffled_darwin_input_gpu;
    float * sleepy_mccarthy_output_gpu;
    float * agitated_johnson_output_gpu;

    cudaMalloc(&vigorous_wing_input_gpu, size);
    cudaMalloc(&unruffled_darwin_input_gpu, size);
    cudaMalloc(&sleepy_mccarthy_output_gpu, size);
    cudaMalloc(&agitated_johnson_output_gpu, size);

    cudaMemcpy(vigorous_wing_input_gpu, vigorous_wing_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(unruffled_darwin_input_gpu, unruffled_darwin_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vigorous_wing_input_gpu, unruffled_darwin_input_gpu, sleepy_mccarthy_output_gpu, agitated_johnson_output_gpu, N);

    cudaMemcpy(sleepy_mccarthy_output, sleepy_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_johnson_output, agitated_johnson_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vigorous_wing_input_gpu);
    cudaFree(unruffled_darwin_input_gpu);
    cudaFree(sleepy_mccarthy_output_gpu);
    cudaFree(agitated_johnson_output_gpu);
}
        ","
void func_cpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){

    for (int eloquent_kepler = 0; eloquent_kepler <= N; eloquent_kepler++){

        float *nifty_gates;
        nifty_gates = (float *)malloc(N*sizeof(float));

    }
        
}
        ",0.030000,0.180000,0,10000,10000,1,1,1,1,1
80,"


__global__ void kernel(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int eloquent_kepler = bx*bdx + tx;
    if (eloquent_kepler < N){
 
        __shared__ float nifty_gates[1024];

    }
    
        
}

void func_gpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vigorous_wing_input_gpu;
    float * unruffled_darwin_input_gpu;
    float * sleepy_mccarthy_output_gpu;
    float * agitated_johnson_output_gpu;

    cudaMalloc(&vigorous_wing_input_gpu, size);
    cudaMalloc(&unruffled_darwin_input_gpu, size);
    cudaMalloc(&sleepy_mccarthy_output_gpu, size);
    cudaMalloc(&agitated_johnson_output_gpu, size);

    cudaMemcpy(vigorous_wing_input_gpu, vigorous_wing_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(unruffled_darwin_input_gpu, unruffled_darwin_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vigorous_wing_input_gpu, unruffled_darwin_input_gpu, sleepy_mccarthy_output_gpu, agitated_johnson_output_gpu, N);

    cudaMemcpy(sleepy_mccarthy_output, sleepy_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_johnson_output, agitated_johnson_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vigorous_wing_input_gpu);
    cudaFree(unruffled_darwin_input_gpu);
    cudaFree(sleepy_mccarthy_output_gpu);
    cudaFree(agitated_johnson_output_gpu);
}
        ","
void func_cpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){

    for (int eloquent_kepler = 0; eloquent_kepler <= N; eloquent_kepler++){

        float *nifty_gates;
        nifty_gates = (float *)malloc(N*sizeof(float));

    }
        
}
        ",0.030000,0.160000,0,10000,1000,1,1,10,1,1
81,"


__global__ void kernel(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int eloquent_kepler = bx*bdx + tx;
    if (eloquent_kepler < N){
 
        __shared__ float nifty_gates[1024];

    }
    
        
}

void func_gpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vigorous_wing_input_gpu;
    float * unruffled_darwin_input_gpu;
    float * sleepy_mccarthy_output_gpu;
    float * agitated_johnson_output_gpu;

    cudaMalloc(&vigorous_wing_input_gpu, size);
    cudaMalloc(&unruffled_darwin_input_gpu, size);
    cudaMalloc(&sleepy_mccarthy_output_gpu, size);
    cudaMalloc(&agitated_johnson_output_gpu, size);

    cudaMemcpy(vigorous_wing_input_gpu, vigorous_wing_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(unruffled_darwin_input_gpu, unruffled_darwin_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vigorous_wing_input_gpu, unruffled_darwin_input_gpu, sleepy_mccarthy_output_gpu, agitated_johnson_output_gpu, N);

    cudaMemcpy(sleepy_mccarthy_output, sleepy_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_johnson_output, agitated_johnson_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vigorous_wing_input_gpu);
    cudaFree(unruffled_darwin_input_gpu);
    cudaFree(sleepy_mccarthy_output_gpu);
    cudaFree(agitated_johnson_output_gpu);
}
        ","
void func_cpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){

    for (int eloquent_kepler = 0; eloquent_kepler <= N; eloquent_kepler++){

        float *nifty_gates;
        nifty_gates = (float *)malloc(N*sizeof(float));

    }
        
}
        ",0.020000,0.160000,0,10000,100,1,1,100,1,1
82,"


__global__ void kernel(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int eloquent_kepler = bx*bdx + tx;
    if (eloquent_kepler < N){
 
        __shared__ float nifty_gates[1024];

    }
    
        
}

void func_gpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vigorous_wing_input_gpu;
    float * unruffled_darwin_input_gpu;
    float * sleepy_mccarthy_output_gpu;
    float * agitated_johnson_output_gpu;

    cudaMalloc(&vigorous_wing_input_gpu, size);
    cudaMalloc(&unruffled_darwin_input_gpu, size);
    cudaMalloc(&sleepy_mccarthy_output_gpu, size);
    cudaMalloc(&agitated_johnson_output_gpu, size);

    cudaMemcpy(vigorous_wing_input_gpu, vigorous_wing_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(unruffled_darwin_input_gpu, unruffled_darwin_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vigorous_wing_input_gpu, unruffled_darwin_input_gpu, sleepy_mccarthy_output_gpu, agitated_johnson_output_gpu, N);

    cudaMemcpy(sleepy_mccarthy_output, sleepy_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_johnson_output, agitated_johnson_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vigorous_wing_input_gpu);
    cudaFree(unruffled_darwin_input_gpu);
    cudaFree(sleepy_mccarthy_output_gpu);
    cudaFree(agitated_johnson_output_gpu);
}
        ","
void func_cpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){

    for (int eloquent_kepler = 0; eloquent_kepler <= N; eloquent_kepler++){

        float *nifty_gates;
        nifty_gates = (float *)malloc(N*sizeof(float));

    }
        
}
        ",0.410000,0.140000,0,100000,10000,1,1,10,1,1
83,"


__global__ void kernel(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int eloquent_kepler = bx*bdx + tx;
    if (eloquent_kepler < N){
 
        __shared__ float nifty_gates[1024];

    }
    
        
}

void func_gpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vigorous_wing_input_gpu;
    float * unruffled_darwin_input_gpu;
    float * sleepy_mccarthy_output_gpu;
    float * agitated_johnson_output_gpu;

    cudaMalloc(&vigorous_wing_input_gpu, size);
    cudaMalloc(&unruffled_darwin_input_gpu, size);
    cudaMalloc(&sleepy_mccarthy_output_gpu, size);
    cudaMalloc(&agitated_johnson_output_gpu, size);

    cudaMemcpy(vigorous_wing_input_gpu, vigorous_wing_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(unruffled_darwin_input_gpu, unruffled_darwin_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vigorous_wing_input_gpu, unruffled_darwin_input_gpu, sleepy_mccarthy_output_gpu, agitated_johnson_output_gpu, N);

    cudaMemcpy(sleepy_mccarthy_output, sleepy_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_johnson_output, agitated_johnson_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vigorous_wing_input_gpu);
    cudaFree(unruffled_darwin_input_gpu);
    cudaFree(sleepy_mccarthy_output_gpu);
    cudaFree(agitated_johnson_output_gpu);
}
        ","
void func_cpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){

    for (int eloquent_kepler = 0; eloquent_kepler <= N; eloquent_kepler++){

        float *nifty_gates;
        nifty_gates = (float *)malloc(N*sizeof(float));

    }
        
}
        ",0.440000,0.170000,0,100000,1000,1,1,100,1,1
84,"


__global__ void kernel(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int eloquent_kepler = bx*bdx + tx;
    if (eloquent_kepler < N){
 
        __shared__ float nifty_gates[1024];

    }
    
        
}

void func_gpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vigorous_wing_input_gpu;
    float * unruffled_darwin_input_gpu;
    float * sleepy_mccarthy_output_gpu;
    float * agitated_johnson_output_gpu;

    cudaMalloc(&vigorous_wing_input_gpu, size);
    cudaMalloc(&unruffled_darwin_input_gpu, size);
    cudaMalloc(&sleepy_mccarthy_output_gpu, size);
    cudaMalloc(&agitated_johnson_output_gpu, size);

    cudaMemcpy(vigorous_wing_input_gpu, vigorous_wing_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(unruffled_darwin_input_gpu, unruffled_darwin_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vigorous_wing_input_gpu, unruffled_darwin_input_gpu, sleepy_mccarthy_output_gpu, agitated_johnson_output_gpu, N);

    cudaMemcpy(sleepy_mccarthy_output, sleepy_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_johnson_output, agitated_johnson_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vigorous_wing_input_gpu);
    cudaFree(unruffled_darwin_input_gpu);
    cudaFree(sleepy_mccarthy_output_gpu);
    cudaFree(agitated_johnson_output_gpu);
}
        ","
void func_cpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){

    for (int eloquent_kepler = 0; eloquent_kepler <= N; eloquent_kepler++){

        float *nifty_gates;
        nifty_gates = (float *)malloc(N*sizeof(float));

    }
        
}
        ",0.460000,0.150000,0,100000,100,1,1,1000,1,1
85,"


__global__ void kernel(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int eloquent_kepler = bx*bdx + tx;
    if (eloquent_kepler < N){
 
        __shared__ float nifty_gates[1024];

    }
    
        
}

void func_gpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vigorous_wing_input_gpu;
    float * unruffled_darwin_input_gpu;
    float * sleepy_mccarthy_output_gpu;
    float * agitated_johnson_output_gpu;

    cudaMalloc(&vigorous_wing_input_gpu, size);
    cudaMalloc(&unruffled_darwin_input_gpu, size);
    cudaMalloc(&sleepy_mccarthy_output_gpu, size);
    cudaMalloc(&agitated_johnson_output_gpu, size);

    cudaMemcpy(vigorous_wing_input_gpu, vigorous_wing_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(unruffled_darwin_input_gpu, unruffled_darwin_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vigorous_wing_input_gpu, unruffled_darwin_input_gpu, sleepy_mccarthy_output_gpu, agitated_johnson_output_gpu, N);

    cudaMemcpy(sleepy_mccarthy_output, sleepy_mccarthy_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(agitated_johnson_output, agitated_johnson_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vigorous_wing_input_gpu);
    cudaFree(unruffled_darwin_input_gpu);
    cudaFree(sleepy_mccarthy_output_gpu);
    cudaFree(agitated_johnson_output_gpu);
}
        ","
void func_cpu(float * vigorous_wing_input, float * unruffled_darwin_input, float * sleepy_mccarthy_output, float * agitated_johnson_output, unsigned int N){

    for (int eloquent_kepler = 0; eloquent_kepler <= N; eloquent_kepler++){

        float *nifty_gates;
        nifty_gates = (float *)malloc(N*sizeof(float));

    }
        
}
        ",0.440000,0.160000,0,100000,1000,1,1,100,1,1
86,"


__global__ void kernel(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int practical_feynman = bx*bdx + tx;
    if (practical_feynman < N){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];
        __syncthreads();
    }
    
        
}

void func_gpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vibrant_edison_input_gpu;
    float * ecstatic_wiles_input_gpu;
    float * practical_sinoussi_output_gpu;

    cudaMalloc(&vibrant_edison_input_gpu, size);
    cudaMalloc(&ecstatic_wiles_input_gpu, size);
    cudaMalloc(&practical_sinoussi_output_gpu, size);

    cudaMemcpy(vibrant_edison_input_gpu, vibrant_edison_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(ecstatic_wiles_input_gpu, ecstatic_wiles_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vibrant_edison_input_gpu, ecstatic_wiles_input_gpu, practical_sinoussi_output_gpu, N);

    cudaMemcpy(practical_sinoussi_output, practical_sinoussi_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vibrant_edison_input_gpu);
    cudaFree(ecstatic_wiles_input_gpu);
    cudaFree(practical_sinoussi_output_gpu);
}
        ","
void func_cpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){

    for (int practical_feynman = 0; practical_feynman <= N; practical_feynman++){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];

    }
        
}
        ",0.000000,0.230000,0,10,10,1,1,1,1,1
87,"


__global__ void kernel(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int practical_feynman = bx*bdx + tx;
    if (practical_feynman < N){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];
        __syncthreads();
    }
    
        
}

void func_gpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vibrant_edison_input_gpu;
    float * ecstatic_wiles_input_gpu;
    float * practical_sinoussi_output_gpu;

    cudaMalloc(&vibrant_edison_input_gpu, size);
    cudaMalloc(&ecstatic_wiles_input_gpu, size);
    cudaMalloc(&practical_sinoussi_output_gpu, size);

    cudaMemcpy(vibrant_edison_input_gpu, vibrant_edison_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(ecstatic_wiles_input_gpu, ecstatic_wiles_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vibrant_edison_input_gpu, ecstatic_wiles_input_gpu, practical_sinoussi_output_gpu, N);

    cudaMemcpy(practical_sinoussi_output, practical_sinoussi_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vibrant_edison_input_gpu);
    cudaFree(ecstatic_wiles_input_gpu);
    cudaFree(practical_sinoussi_output_gpu);
}
        ","
void func_cpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){

    for (int practical_feynman = 0; practical_feynman <= N; practical_feynman++){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];

    }
        
}
        ",0.000000,0.160000,0,100,100,1,1,1,1,1
88,"


__global__ void kernel(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int practical_feynman = bx*bdx + tx;
    if (practical_feynman < N){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];
        __syncthreads();
    }
    
        
}

void func_gpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vibrant_edison_input_gpu;
    float * ecstatic_wiles_input_gpu;
    float * practical_sinoussi_output_gpu;

    cudaMalloc(&vibrant_edison_input_gpu, size);
    cudaMalloc(&ecstatic_wiles_input_gpu, size);
    cudaMalloc(&practical_sinoussi_output_gpu, size);

    cudaMemcpy(vibrant_edison_input_gpu, vibrant_edison_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(ecstatic_wiles_input_gpu, ecstatic_wiles_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vibrant_edison_input_gpu, ecstatic_wiles_input_gpu, practical_sinoussi_output_gpu, N);

    cudaMemcpy(practical_sinoussi_output, practical_sinoussi_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vibrant_edison_input_gpu);
    cudaFree(ecstatic_wiles_input_gpu);
    cudaFree(practical_sinoussi_output_gpu);
}
        ","
void func_cpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){

    for (int practical_feynman = 0; practical_feynman <= N; practical_feynman++){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];

    }
        
}
        ",0.000000,0.150000,0,100,10,1,1,10,1,1
89,"


__global__ void kernel(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int practical_feynman = bx*bdx + tx;
    if (practical_feynman < N){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];
        __syncthreads();
    }
    
        
}

void func_gpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vibrant_edison_input_gpu;
    float * ecstatic_wiles_input_gpu;
    float * practical_sinoussi_output_gpu;

    cudaMalloc(&vibrant_edison_input_gpu, size);
    cudaMalloc(&ecstatic_wiles_input_gpu, size);
    cudaMalloc(&practical_sinoussi_output_gpu, size);

    cudaMemcpy(vibrant_edison_input_gpu, vibrant_edison_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(ecstatic_wiles_input_gpu, ecstatic_wiles_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vibrant_edison_input_gpu, ecstatic_wiles_input_gpu, practical_sinoussi_output_gpu, N);

    cudaMemcpy(practical_sinoussi_output, practical_sinoussi_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vibrant_edison_input_gpu);
    cudaFree(ecstatic_wiles_input_gpu);
    cudaFree(practical_sinoussi_output_gpu);
}
        ","
void func_cpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){

    for (int practical_feynman = 0; practical_feynman <= N; practical_feynman++){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];

    }
        
}
        ",0.000000,0.150000,0,100,1,1,1,100,1,1
90,"


__global__ void kernel(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int practical_feynman = bx*bdx + tx;
    if (practical_feynman < N){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];
        __syncthreads();
    }
    
        
}

void func_gpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vibrant_edison_input_gpu;
    float * ecstatic_wiles_input_gpu;
    float * practical_sinoussi_output_gpu;

    cudaMalloc(&vibrant_edison_input_gpu, size);
    cudaMalloc(&ecstatic_wiles_input_gpu, size);
    cudaMalloc(&practical_sinoussi_output_gpu, size);

    cudaMemcpy(vibrant_edison_input_gpu, vibrant_edison_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(ecstatic_wiles_input_gpu, ecstatic_wiles_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vibrant_edison_input_gpu, ecstatic_wiles_input_gpu, practical_sinoussi_output_gpu, N);

    cudaMemcpy(practical_sinoussi_output, practical_sinoussi_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vibrant_edison_input_gpu);
    cudaFree(ecstatic_wiles_input_gpu);
    cudaFree(practical_sinoussi_output_gpu);
}
        ","
void func_cpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){

    for (int practical_feynman = 0; practical_feynman <= N; practical_feynman++){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];

    }
        
}
        ",0.000000,0.160000,0,10000,10000,1,1,1,1,1
91,"


__global__ void kernel(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int practical_feynman = bx*bdx + tx;
    if (practical_feynman < N){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];
        __syncthreads();
    }
    
        
}

void func_gpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vibrant_edison_input_gpu;
    float * ecstatic_wiles_input_gpu;
    float * practical_sinoussi_output_gpu;

    cudaMalloc(&vibrant_edison_input_gpu, size);
    cudaMalloc(&ecstatic_wiles_input_gpu, size);
    cudaMalloc(&practical_sinoussi_output_gpu, size);

    cudaMemcpy(vibrant_edison_input_gpu, vibrant_edison_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(ecstatic_wiles_input_gpu, ecstatic_wiles_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vibrant_edison_input_gpu, ecstatic_wiles_input_gpu, practical_sinoussi_output_gpu, N);

    cudaMemcpy(practical_sinoussi_output, practical_sinoussi_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vibrant_edison_input_gpu);
    cudaFree(ecstatic_wiles_input_gpu);
    cudaFree(practical_sinoussi_output_gpu);
}
        ","
void func_cpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){

    for (int practical_feynman = 0; practical_feynman <= N; practical_feynman++){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];

    }
        
}
        ",0.000000,0.160000,0,10000,1000,1,1,10,1,1
92,"


__global__ void kernel(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int practical_feynman = bx*bdx + tx;
    if (practical_feynman < N){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];
        __syncthreads();
    }
    
        
}

void func_gpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vibrant_edison_input_gpu;
    float * ecstatic_wiles_input_gpu;
    float * practical_sinoussi_output_gpu;

    cudaMalloc(&vibrant_edison_input_gpu, size);
    cudaMalloc(&ecstatic_wiles_input_gpu, size);
    cudaMalloc(&practical_sinoussi_output_gpu, size);

    cudaMemcpy(vibrant_edison_input_gpu, vibrant_edison_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(ecstatic_wiles_input_gpu, ecstatic_wiles_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vibrant_edison_input_gpu, ecstatic_wiles_input_gpu, practical_sinoussi_output_gpu, N);

    cudaMemcpy(practical_sinoussi_output, practical_sinoussi_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vibrant_edison_input_gpu);
    cudaFree(ecstatic_wiles_input_gpu);
    cudaFree(practical_sinoussi_output_gpu);
}
        ","
void func_cpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){

    for (int practical_feynman = 0; practical_feynman <= N; practical_feynman++){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];

    }
        
}
        ",0.000000,0.190000,0,10000,100,1,1,100,1,1
93,"


__global__ void kernel(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int practical_feynman = bx*bdx + tx;
    if (practical_feynman < N){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];
        __syncthreads();
    }
    
        
}

void func_gpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vibrant_edison_input_gpu;
    float * ecstatic_wiles_input_gpu;
    float * practical_sinoussi_output_gpu;

    cudaMalloc(&vibrant_edison_input_gpu, size);
    cudaMalloc(&ecstatic_wiles_input_gpu, size);
    cudaMalloc(&practical_sinoussi_output_gpu, size);

    cudaMemcpy(vibrant_edison_input_gpu, vibrant_edison_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(ecstatic_wiles_input_gpu, ecstatic_wiles_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vibrant_edison_input_gpu, ecstatic_wiles_input_gpu, practical_sinoussi_output_gpu, N);

    cudaMemcpy(practical_sinoussi_output, practical_sinoussi_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vibrant_edison_input_gpu);
    cudaFree(ecstatic_wiles_input_gpu);
    cudaFree(practical_sinoussi_output_gpu);
}
        ","
void func_cpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){

    for (int practical_feynman = 0; practical_feynman <= N; practical_feynman++){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];

    }
        
}
        ",0.000000,0.170000,0,100000,10000,1,1,10,1,1
94,"


__global__ void kernel(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int practical_feynman = bx*bdx + tx;
    if (practical_feynman < N){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];
        __syncthreads();
    }
    
        
}

void func_gpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vibrant_edison_input_gpu;
    float * ecstatic_wiles_input_gpu;
    float * practical_sinoussi_output_gpu;

    cudaMalloc(&vibrant_edison_input_gpu, size);
    cudaMalloc(&ecstatic_wiles_input_gpu, size);
    cudaMalloc(&practical_sinoussi_output_gpu, size);

    cudaMemcpy(vibrant_edison_input_gpu, vibrant_edison_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(ecstatic_wiles_input_gpu, ecstatic_wiles_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vibrant_edison_input_gpu, ecstatic_wiles_input_gpu, practical_sinoussi_output_gpu, N);

    cudaMemcpy(practical_sinoussi_output, practical_sinoussi_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vibrant_edison_input_gpu);
    cudaFree(ecstatic_wiles_input_gpu);
    cudaFree(practical_sinoussi_output_gpu);
}
        ","
void func_cpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){

    for (int practical_feynman = 0; practical_feynman <= N; practical_feynman++){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];

    }
        
}
        ",0.000000,0.160000,0,100000,1000,1,1,100,1,1
95,"


__global__ void kernel(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int practical_feynman = bx*bdx + tx;
    if (practical_feynman < N){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];
        __syncthreads();
    }
    
        
}

void func_gpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vibrant_edison_input_gpu;
    float * ecstatic_wiles_input_gpu;
    float * practical_sinoussi_output_gpu;

    cudaMalloc(&vibrant_edison_input_gpu, size);
    cudaMalloc(&ecstatic_wiles_input_gpu, size);
    cudaMalloc(&practical_sinoussi_output_gpu, size);

    cudaMemcpy(vibrant_edison_input_gpu, vibrant_edison_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(ecstatic_wiles_input_gpu, ecstatic_wiles_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vibrant_edison_input_gpu, ecstatic_wiles_input_gpu, practical_sinoussi_output_gpu, N);

    cudaMemcpy(practical_sinoussi_output, practical_sinoussi_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vibrant_edison_input_gpu);
    cudaFree(ecstatic_wiles_input_gpu);
    cudaFree(practical_sinoussi_output_gpu);
}
        ","
void func_cpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){

    for (int practical_feynman = 0; practical_feynman <= N; practical_feynman++){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];

    }
        
}
        ",0.000000,0.170000,0,100000,100,1,1,1000,1,1
96,"


__global__ void kernel(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;

    int practical_feynman = bx*bdx + tx;
    if (practical_feynman < N){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];
        __syncthreads();
    }
    
        
}

void func_gpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * vibrant_edison_input_gpu;
    float * ecstatic_wiles_input_gpu;
    float * practical_sinoussi_output_gpu;

    cudaMalloc(&vibrant_edison_input_gpu, size);
    cudaMalloc(&ecstatic_wiles_input_gpu, size);
    cudaMalloc(&practical_sinoussi_output_gpu, size);

    cudaMemcpy(vibrant_edison_input_gpu, vibrant_edison_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(ecstatic_wiles_input_gpu, ecstatic_wiles_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(vibrant_edison_input_gpu, ecstatic_wiles_input_gpu, practical_sinoussi_output_gpu, N);

    cudaMemcpy(practical_sinoussi_output, practical_sinoussi_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(vibrant_edison_input_gpu);
    cudaFree(ecstatic_wiles_input_gpu);
    cudaFree(practical_sinoussi_output_gpu);
}
        ","
void func_cpu(float * vibrant_edison_input, float * ecstatic_wiles_input, float * practical_sinoussi_output, unsigned int N){

    for (int practical_feynman = 0; practical_feynman <= N; practical_feynman++){
        practical_sinoussi_output[practical_feynman] -= vibrant_edison_input[practical_feynman];
        practical_sinoussi_output[practical_feynman] /= vibrant_edison_input[practical_feynman];

    }
        
}
        ",0.000000,0.180000,0,100000,1000,1,1,100,1,1
97,"


__global__ void kernel(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int lucid_hodgkin = bx*bdx + tx;
    if (lucid_hodgkin < N){

        int thirsty_mcnulty = by*bdy + ty;
        if (thirsty_mcnulty < N){
 
            __shared__ float festive_fermi[1024];

        }
        
        
        __syncthreads();
    }
    
        
}

void func_gpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * inspiring_kirch_input_gpu;
    float * laughing_mestorf_input_gpu;
    float * practical_lichterman_output_gpu;
    float * hungry_ardinghelli_output_gpu;

    cudaMalloc(&inspiring_kirch_input_gpu, size);
    cudaMalloc(&laughing_mestorf_input_gpu, size);
    cudaMalloc(&practical_lichterman_output_gpu, size);
    cudaMalloc(&hungry_ardinghelli_output_gpu, size);

    cudaMemcpy(inspiring_kirch_input_gpu, inspiring_kirch_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(laughing_mestorf_input_gpu, laughing_mestorf_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(inspiring_kirch_input_gpu, laughing_mestorf_input_gpu, practical_lichterman_output_gpu, hungry_ardinghelli_output_gpu, N);

    cudaMemcpy(practical_lichterman_output, practical_lichterman_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(hungry_ardinghelli_output, hungry_ardinghelli_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(inspiring_kirch_input_gpu);
    cudaFree(laughing_mestorf_input_gpu);
    cudaFree(practical_lichterman_output_gpu);
    cudaFree(hungry_ardinghelli_output_gpu);
}
        ","
void func_cpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){

    for (int lucid_hodgkin = 0; lucid_hodgkin <= N; lucid_hodgkin++){

        for (int thirsty_mcnulty = 0; thirsty_mcnulty <= N; thirsty_mcnulty++){

            float *festive_fermi;
            festive_fermi = (float *)malloc(N*sizeof(float));

        }
        

    }
        
}
        ",0.000000,0.160000,0,10,10,10,1,1,1,1
98,"


__global__ void kernel(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int lucid_hodgkin = bx*bdx + tx;
    if (lucid_hodgkin < N){

        int thirsty_mcnulty = by*bdy + ty;
        if (thirsty_mcnulty < N){
 
            __shared__ float festive_fermi[1024];

        }
        
        
        __syncthreads();
    }
    
        
}

void func_gpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * inspiring_kirch_input_gpu;
    float * laughing_mestorf_input_gpu;
    float * practical_lichterman_output_gpu;
    float * hungry_ardinghelli_output_gpu;

    cudaMalloc(&inspiring_kirch_input_gpu, size);
    cudaMalloc(&laughing_mestorf_input_gpu, size);
    cudaMalloc(&practical_lichterman_output_gpu, size);
    cudaMalloc(&hungry_ardinghelli_output_gpu, size);

    cudaMemcpy(inspiring_kirch_input_gpu, inspiring_kirch_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(laughing_mestorf_input_gpu, laughing_mestorf_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(inspiring_kirch_input_gpu, laughing_mestorf_input_gpu, practical_lichterman_output_gpu, hungry_ardinghelli_output_gpu, N);

    cudaMemcpy(practical_lichterman_output, practical_lichterman_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(hungry_ardinghelli_output, hungry_ardinghelli_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(inspiring_kirch_input_gpu);
    cudaFree(laughing_mestorf_input_gpu);
    cudaFree(practical_lichterman_output_gpu);
    cudaFree(hungry_ardinghelli_output_gpu);
}
        ","
void func_cpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){

    for (int lucid_hodgkin = 0; lucid_hodgkin <= N; lucid_hodgkin++){

        for (int thirsty_mcnulty = 0; thirsty_mcnulty <= N; thirsty_mcnulty++){

            float *festive_fermi;
            festive_fermi = (float *)malloc(N*sizeof(float));

        }
        

    }
        
}
        ",0.000000,0.170000,0,100,100,100,1,1,1,1
99,"


__global__ void kernel(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int lucid_hodgkin = bx*bdx + tx;
    if (lucid_hodgkin < N){

        int thirsty_mcnulty = by*bdy + ty;
        if (thirsty_mcnulty < N){
 
            __shared__ float festive_fermi[1024];

        }
        
        
        __syncthreads();
    }
    
        
}

void func_gpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * inspiring_kirch_input_gpu;
    float * laughing_mestorf_input_gpu;
    float * practical_lichterman_output_gpu;
    float * hungry_ardinghelli_output_gpu;

    cudaMalloc(&inspiring_kirch_input_gpu, size);
    cudaMalloc(&laughing_mestorf_input_gpu, size);
    cudaMalloc(&practical_lichterman_output_gpu, size);
    cudaMalloc(&hungry_ardinghelli_output_gpu, size);

    cudaMemcpy(inspiring_kirch_input_gpu, inspiring_kirch_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(laughing_mestorf_input_gpu, laughing_mestorf_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(inspiring_kirch_input_gpu, laughing_mestorf_input_gpu, practical_lichterman_output_gpu, hungry_ardinghelli_output_gpu, N);

    cudaMemcpy(practical_lichterman_output, practical_lichterman_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(hungry_ardinghelli_output, hungry_ardinghelli_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(inspiring_kirch_input_gpu);
    cudaFree(laughing_mestorf_input_gpu);
    cudaFree(practical_lichterman_output_gpu);
    cudaFree(hungry_ardinghelli_output_gpu);
}
        ","
void func_cpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){

    for (int lucid_hodgkin = 0; lucid_hodgkin <= N; lucid_hodgkin++){

        for (int thirsty_mcnulty = 0; thirsty_mcnulty <= N; thirsty_mcnulty++){

            float *festive_fermi;
            festive_fermi = (float *)malloc(N*sizeof(float));

        }
        

    }
        
}
        ",0.000000,0.170000,0,100,10,10,1,10,10,1
100,"


__global__ void kernel(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int lucid_hodgkin = bx*bdx + tx;
    if (lucid_hodgkin < N){

        int thirsty_mcnulty = by*bdy + ty;
        if (thirsty_mcnulty < N){
 
            __shared__ float festive_fermi[1024];

        }
        
        
        __syncthreads();
    }
    
        
}

void func_gpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * inspiring_kirch_input_gpu;
    float * laughing_mestorf_input_gpu;
    float * practical_lichterman_output_gpu;
    float * hungry_ardinghelli_output_gpu;

    cudaMalloc(&inspiring_kirch_input_gpu, size);
    cudaMalloc(&laughing_mestorf_input_gpu, size);
    cudaMalloc(&practical_lichterman_output_gpu, size);
    cudaMalloc(&hungry_ardinghelli_output_gpu, size);

    cudaMemcpy(inspiring_kirch_input_gpu, inspiring_kirch_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(laughing_mestorf_input_gpu, laughing_mestorf_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(inspiring_kirch_input_gpu, laughing_mestorf_input_gpu, practical_lichterman_output_gpu, hungry_ardinghelli_output_gpu, N);

    cudaMemcpy(practical_lichterman_output, practical_lichterman_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(hungry_ardinghelli_output, hungry_ardinghelli_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(inspiring_kirch_input_gpu);
    cudaFree(laughing_mestorf_input_gpu);
    cudaFree(practical_lichterman_output_gpu);
    cudaFree(hungry_ardinghelli_output_gpu);
}
        ","
void func_cpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){

    for (int lucid_hodgkin = 0; lucid_hodgkin <= N; lucid_hodgkin++){

        for (int thirsty_mcnulty = 0; thirsty_mcnulty <= N; thirsty_mcnulty++){

            float *festive_fermi;
            festive_fermi = (float *)malloc(N*sizeof(float));

        }
        

    }
        
}
        ",0.000000,0.180000,0,100,1,1,1,100,100,1
101,"


__global__ void kernel(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int lucid_hodgkin = bx*bdx + tx;
    if (lucid_hodgkin < N){

        int thirsty_mcnulty = by*bdy + ty;
        if (thirsty_mcnulty < N){
 
            __shared__ float festive_fermi[1024];

        }
        
        
        __syncthreads();
    }
    
        
}

void func_gpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * inspiring_kirch_input_gpu;
    float * laughing_mestorf_input_gpu;
    float * practical_lichterman_output_gpu;
    float * hungry_ardinghelli_output_gpu;

    cudaMalloc(&inspiring_kirch_input_gpu, size);
    cudaMalloc(&laughing_mestorf_input_gpu, size);
    cudaMalloc(&practical_lichterman_output_gpu, size);
    cudaMalloc(&hungry_ardinghelli_output_gpu, size);

    cudaMemcpy(inspiring_kirch_input_gpu, inspiring_kirch_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(laughing_mestorf_input_gpu, laughing_mestorf_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(inspiring_kirch_input_gpu, laughing_mestorf_input_gpu, practical_lichterman_output_gpu, hungry_ardinghelli_output_gpu, N);

    cudaMemcpy(practical_lichterman_output, practical_lichterman_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(hungry_ardinghelli_output, hungry_ardinghelli_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(inspiring_kirch_input_gpu);
    cudaFree(laughing_mestorf_input_gpu);
    cudaFree(practical_lichterman_output_gpu);
    cudaFree(hungry_ardinghelli_output_gpu);
}
        ","
void func_cpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){

    for (int lucid_hodgkin = 0; lucid_hodgkin <= N; lucid_hodgkin++){

        for (int thirsty_mcnulty = 0; thirsty_mcnulty <= N; thirsty_mcnulty++){

            float *festive_fermi;
            festive_fermi = (float *)malloc(N*sizeof(float));

        }
        

    }
        
}
        ",2.790000,0.210000,0,1000,1000,1000,1,1,1,1
102,"


__global__ void kernel(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int lucid_hodgkin = bx*bdx + tx;
    if (lucid_hodgkin < N){

        int thirsty_mcnulty = by*bdy + ty;
        if (thirsty_mcnulty < N){
 
            __shared__ float festive_fermi[1024];

        }
        
        
        __syncthreads();
    }
    
        
}

void func_gpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * inspiring_kirch_input_gpu;
    float * laughing_mestorf_input_gpu;
    float * practical_lichterman_output_gpu;
    float * hungry_ardinghelli_output_gpu;

    cudaMalloc(&inspiring_kirch_input_gpu, size);
    cudaMalloc(&laughing_mestorf_input_gpu, size);
    cudaMalloc(&practical_lichterman_output_gpu, size);
    cudaMalloc(&hungry_ardinghelli_output_gpu, size);

    cudaMemcpy(inspiring_kirch_input_gpu, inspiring_kirch_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(laughing_mestorf_input_gpu, laughing_mestorf_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(inspiring_kirch_input_gpu, laughing_mestorf_input_gpu, practical_lichterman_output_gpu, hungry_ardinghelli_output_gpu, N);

    cudaMemcpy(practical_lichterman_output, practical_lichterman_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(hungry_ardinghelli_output, hungry_ardinghelli_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(inspiring_kirch_input_gpu);
    cudaFree(laughing_mestorf_input_gpu);
    cudaFree(practical_lichterman_output_gpu);
    cudaFree(hungry_ardinghelli_output_gpu);
}
        ","
void func_cpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){

    for (int lucid_hodgkin = 0; lucid_hodgkin <= N; lucid_hodgkin++){

        for (int thirsty_mcnulty = 0; thirsty_mcnulty <= N; thirsty_mcnulty++){

            float *festive_fermi;
            festive_fermi = (float *)malloc(N*sizeof(float));

        }
        

    }
        
}
        ",2.800000,0.240000,0,1000,100,100,1,10,10,1
103,"


__global__ void kernel(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int lucid_hodgkin = bx*bdx + tx;
    if (lucid_hodgkin < N){

        int thirsty_mcnulty = by*bdy + ty;
        if (thirsty_mcnulty < N){
 
            __shared__ float festive_fermi[1024];

        }
        
        
        __syncthreads();
    }
    
        
}

void func_gpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * inspiring_kirch_input_gpu;
    float * laughing_mestorf_input_gpu;
    float * practical_lichterman_output_gpu;
    float * hungry_ardinghelli_output_gpu;

    cudaMalloc(&inspiring_kirch_input_gpu, size);
    cudaMalloc(&laughing_mestorf_input_gpu, size);
    cudaMalloc(&practical_lichterman_output_gpu, size);
    cudaMalloc(&hungry_ardinghelli_output_gpu, size);

    cudaMemcpy(inspiring_kirch_input_gpu, inspiring_kirch_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(laughing_mestorf_input_gpu, laughing_mestorf_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(inspiring_kirch_input_gpu, laughing_mestorf_input_gpu, practical_lichterman_output_gpu, hungry_ardinghelli_output_gpu, N);

    cudaMemcpy(practical_lichterman_output, practical_lichterman_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(hungry_ardinghelli_output, hungry_ardinghelli_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(inspiring_kirch_input_gpu);
    cudaFree(laughing_mestorf_input_gpu);
    cudaFree(practical_lichterman_output_gpu);
    cudaFree(hungry_ardinghelli_output_gpu);
}
        ","
void func_cpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){

    for (int lucid_hodgkin = 0; lucid_hodgkin <= N; lucid_hodgkin++){

        for (int thirsty_mcnulty = 0; thirsty_mcnulty <= N; thirsty_mcnulty++){

            float *festive_fermi;
            festive_fermi = (float *)malloc(N*sizeof(float));

        }
        

    }
        
}
        ",73.080000,0.360000,0,5000,5000,5000,1,1,1,1
104,"


__global__ void kernel(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int lucid_hodgkin = bx*bdx + tx;
    if (lucid_hodgkin < N){

        int thirsty_mcnulty = by*bdy + ty;
        if (thirsty_mcnulty < N){
 
            __shared__ float festive_fermi[1024];

        }
        
        
        __syncthreads();
    }
    
        
}

void func_gpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * inspiring_kirch_input_gpu;
    float * laughing_mestorf_input_gpu;
    float * practical_lichterman_output_gpu;
    float * hungry_ardinghelli_output_gpu;

    cudaMalloc(&inspiring_kirch_input_gpu, size);
    cudaMalloc(&laughing_mestorf_input_gpu, size);
    cudaMalloc(&practical_lichterman_output_gpu, size);
    cudaMalloc(&hungry_ardinghelli_output_gpu, size);

    cudaMemcpy(inspiring_kirch_input_gpu, inspiring_kirch_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(laughing_mestorf_input_gpu, laughing_mestorf_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(inspiring_kirch_input_gpu, laughing_mestorf_input_gpu, practical_lichterman_output_gpu, hungry_ardinghelli_output_gpu, N);

    cudaMemcpy(practical_lichterman_output, practical_lichterman_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(hungry_ardinghelli_output, hungry_ardinghelli_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(inspiring_kirch_input_gpu);
    cudaFree(laughing_mestorf_input_gpu);
    cudaFree(practical_lichterman_output_gpu);
    cudaFree(hungry_ardinghelli_output_gpu);
}
        ","
void func_cpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){

    for (int lucid_hodgkin = 0; lucid_hodgkin <= N; lucid_hodgkin++){

        for (int thirsty_mcnulty = 0; thirsty_mcnulty <= N; thirsty_mcnulty++){

            float *festive_fermi;
            festive_fermi = (float *)malloc(N*sizeof(float));

        }
        

    }
        
}
        ",66.180000,0.290000,0,5000,500,500,1,10,10,1
105,"


__global__ void kernel(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){
    int bx = blockIdx.x, bdx = blockDim.x,  tx = threadIdx.x;
    int by = blockIdx.y, bdy = blockDim.y,  ty = threadIdx.y;

    int lucid_hodgkin = bx*bdx + tx;
    if (lucid_hodgkin < N){

        int thirsty_mcnulty = by*bdy + ty;
        if (thirsty_mcnulty < N){
 
            __shared__ float festive_fermi[1024];

        }
        
        
        __syncthreads();
    }
    
        
}

void func_gpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N, dim3 block, dim3 grid, size_t size){
    float * inspiring_kirch_input_gpu;
    float * laughing_mestorf_input_gpu;
    float * practical_lichterman_output_gpu;
    float * hungry_ardinghelli_output_gpu;

    cudaMalloc(&inspiring_kirch_input_gpu, size);
    cudaMalloc(&laughing_mestorf_input_gpu, size);
    cudaMalloc(&practical_lichterman_output_gpu, size);
    cudaMalloc(&hungry_ardinghelli_output_gpu, size);

    cudaMemcpy(inspiring_kirch_input_gpu, inspiring_kirch_input, size, cudaMemcpyHostToDevice);
    cudaMemcpy(laughing_mestorf_input_gpu, laughing_mestorf_input, size, cudaMemcpyHostToDevice);

    kernel<<<block, grid>>>(inspiring_kirch_input_gpu, laughing_mestorf_input_gpu, practical_lichterman_output_gpu, hungry_ardinghelli_output_gpu, N);

    cudaMemcpy(practical_lichterman_output, practical_lichterman_output_gpu, size, cudaMemcpyDeviceToHost);
    cudaMemcpy(hungry_ardinghelli_output, hungry_ardinghelli_output_gpu, size, cudaMemcpyDeviceToHost);

    cudaFree(inspiring_kirch_input_gpu);
    cudaFree(laughing_mestorf_input_gpu);
    cudaFree(practical_lichterman_output_gpu);
    cudaFree(hungry_ardinghelli_output_gpu);
}
        ","
void func_cpu(float * inspiring_kirch_input, float * laughing_mestorf_input, float * practical_lichterman_output, float * hungry_ardinghelli_output, unsigned int N){

    for (int lucid_hodgkin = 0; lucid_hodgkin <= N; lucid_hodgkin++){

        for (int thirsty_mcnulty = 0; thirsty_mcnulty <= N; thirsty_mcnulty++){

            float *festive_fermi;
            festive_fermi = (float *)malloc(N*sizeof(float));

        }
        

    }
        
}
        ",54.140000,0.240000,0,5000,50,50,1,100,100,1
